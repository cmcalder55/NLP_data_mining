{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Word Vector (a.k.a Word Embedding) </center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Word2Vector\n",
    " - Vector representation of words (i.e. word vectors) learned using neural network\n",
    "   - e.g. \"apple\" : [0.35, -0.2, 0.4, ...], 'mango':  [0.32, -0.18, 0.5, ...]\n",
    "   - Interesting properties of word vectors:\n",
    "    * **Words with similar semantics have close word vectors**\n",
    "    <img src=\"https://www.kdnuggets.com/images/cartoon-espresso-word2vec.jpg\" width=\"50%\">\n",
    "    https://www.kdnuggets.com/2017/04/cartoon-word2vec-espresso-cappuccino.html\n",
    "    * **Composition**: e.g. vector(\"woman\")+vector(\"king\")-vector('man') $\\approx$ vector(\"queen\")\n",
    " - Models:\n",
    "   - **CBOW** (Continuous Bag of Words): Predict a target word based on context\n",
    "     - e.g. the fox jumped over the lazy dog\n",
    "     - Assuming symmetric context with window size 3, this sentence can create training samples: \n",
    "       - ([-, fox], the) \n",
    "       - ([the, jumped], fox) \n",
    "       - ([fox, over], jumped)\n",
    "       - ([jumped, the], over) \n",
    "       - ...\n",
    "       \n",
    "       <img src=\"cbow.png\" width=\"50%\">\n",
    "       source: https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/\n",
    "   - **Skip Gram**: predict context based on target words\n",
    "   \n",
    "        <img src=\"skip_gram.png\" width=\"50%\">\n",
    "        source: https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up interactive shell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>This is a little longer and more detailed than...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Only Michelle Branch save this album!!!!All gu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>A surprisingly good book, given its inherently...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>This is a wonderful, quiet and relaxing CD tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>The lights that I received are absolutely not ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>1</td>\n",
       "      <td>This was suppose to be such a great movie and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>2</td>\n",
       "      <td>I gave this poncho as a birthday gift to my hu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>1</td>\n",
       "      <td>These diapers absorb MUCH less than Swaddlers....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>2</td>\n",
       "      <td>\"Moonstruck\" is one of the best movies of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>1</td>\n",
       "      <td>I thought this would be full of wit and wisdom...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                               text\n",
       "0          2  This is a little longer and more detailed than...\n",
       "1          1  Only Michelle Branch save this album!!!!All gu...\n",
       "2          2  A surprisingly good book, given its inherently...\n",
       "3          2  This is a wonderful, quiet and relaxing CD tha...\n",
       "4          1  The lights that I received are absolutely not ...\n",
       "...      ...                                                ...\n",
       "19995      1  This was suppose to be such a great movie and ...\n",
       "19996      2  I gave this poncho as a birthday gift to my hu...\n",
       "19997      1  These diapers absorb MUCH less than Swaddlers....\n",
       "19998      2  \"Moonstruck\" is one of the best movies of the ...\n",
       "19999      1  I thought this would be full of wit and wisdom...\n",
       "\n",
       "[20000 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['this', 'is', 'little', 'longer', 'and', 'more', 'detailed', 'than', 'the', 'first', 'two', 'books', 'in', 'the', 'series', 'however', 'have', 'enjoyed', 'each', 'new', 'aspect', 'of', 'the', 'exciting', 'fantasy', 'universe'], ['only', 'michelle', 'branch', 'save', 'this', 'album', 'all', 'guys', 'play', 'along', 'with', 'unenthusiastic', 'beat', 'even', 'karl']]\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1.1 Train your word vector\n",
    "\n",
    "import pandas as pd\n",
    "import nltk,string\n",
    "\n",
    "# Load data\n",
    "data=pd.read_csv('amazon_review_large.csv')\n",
    "data.columns=['label','text']\n",
    "data\n",
    "\n",
    "# tokenize each document into a list of unigrams\n",
    "# strip punctuations and leading/trailing spaces from unigrams\n",
    "# only unigrams with 2 or more characters are taken\n",
    "sentences=[ [token.strip(string.punctuation).strip() \\\n",
    "             for token in nltk.word_tokenize(doc.lower()) \\\n",
    "                 if token not in string.punctuation and \\\n",
    "                 len(token.strip(string.punctuation).strip())>=2]\\\n",
    "             for doc in data[\"text\"]]\n",
    "print(sentences[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-12 20:13:53,054 : INFO : collecting all words and their counts\n",
      "2023-04-12 20:13:53,055 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2023-04-12 20:13:53,218 : INFO : PROGRESS: at sentence #10000, processed 711991 words, keeping 36968 word types\n",
      "2023-04-12 20:13:53,351 : INFO : collected 55241 word types from a corpus of 1424289 raw words and 20000 sentences\n",
      "2023-04-12 20:13:53,352 : INFO : Creating a fresh vocabulary\n",
      "2023-04-12 20:13:53,447 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 12133 unique words (21.96375880233884%% of original 55241, drops 43108)', 'datetime': '2023-04-12T20:13:53.420065', 'gensim': '4.1.2', 'python': '3.7.6 (default, Jan  8 2020, 13:42:34) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-20.6.0-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2023-04-12 20:13:53,450 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 1361999 word corpus (95.62658982832838%% of original 1424289, drops 62290)', 'datetime': '2023-04-12T20:13:53.450919', 'gensim': '4.1.2', 'python': '3.7.6 (default, Jan  8 2020, 13:42:34) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-20.6.0-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2023-04-12 20:13:53,542 : INFO : deleting the raw counts dictionary of 55241 items\n",
      "2023-04-12 20:13:53,560 : INFO : sample=0.001 downsamples 57 most-common words\n",
      "2023-04-12 20:13:53,561 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 1015588.3176218013 word corpus (74.6%% of prior 1361999)', 'datetime': '2023-04-12T20:13:53.561083', 'gensim': '4.1.2', 'python': '3.7.6 (default, Jan  8 2020, 13:42:34) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-20.6.0-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2023-04-12 20:13:53,693 : INFO : estimated required memory for 12133 words and 200 dimensions: 25479300 bytes\n",
      "2023-04-12 20:13:53,694 : INFO : resetting layer weights\n",
      "2023-04-12 20:13:53,710 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-04-12T20:13:53.710359', 'gensim': '4.1.2', 'python': '3.7.6 (default, Jan  8 2020, 13:42:34) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-20.6.0-x86_64-i386-64bit', 'event': 'build_vocab'}\n",
      "2023-04-12 20:13:53,711 : INFO : Word2Vec lifecycle event {'msg': 'training model with 4 workers on 12133 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2023-04-12T20:13:53.711150', 'gensim': '4.1.2', 'python': '3.7.6 (default, Jan  8 2020, 13:42:34) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-20.6.0-x86_64-i386-64bit', 'event': 'train'}\n",
      "2023-04-12 20:13:54,298 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2023-04-12 20:13:54,300 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-04-12 20:13:54,305 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-04-12 20:13:54,309 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-04-12 20:13:54,310 : INFO : EPOCH - 1 : training on 1424289 raw words (1015317 effective words) took 0.6s, 1706867 effective words/s\n",
      "2023-04-12 20:13:54,882 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2023-04-12 20:13:54,884 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-04-12 20:13:54,891 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-04-12 20:13:54,894 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-04-12 20:13:54,895 : INFO : EPOCH - 2 : training on 1424289 raw words (1015543 effective words) took 0.6s, 1755164 effective words/s\n",
      "2023-04-12 20:13:55,476 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2023-04-12 20:13:55,480 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-04-12 20:13:55,484 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-04-12 20:13:55,488 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-04-12 20:13:55,489 : INFO : EPOCH - 3 : training on 1424289 raw words (1014934 effective words) took 0.6s, 1727719 effective words/s\n",
      "2023-04-12 20:13:56,034 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2023-04-12 20:13:56,040 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-04-12 20:13:56,043 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-04-12 20:13:56,046 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-04-12 20:13:56,047 : INFO : EPOCH - 4 : training on 1424289 raw words (1015627 effective words) took 0.6s, 1836586 effective words/s\n",
      "2023-04-12 20:13:56,619 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2023-04-12 20:13:56,622 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-04-12 20:13:56,624 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-04-12 20:13:56,628 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-04-12 20:13:56,628 : INFO : EPOCH - 5 : training on 1424289 raw words (1015739 effective words) took 0.6s, 1768551 effective words/s\n",
      "2023-04-12 20:13:56,629 : INFO : Word2Vec lifecycle event {'msg': 'training on 7121445 raw words (5077160 effective words) took 2.9s, 1739999 effective words/s', 'datetime': '2023-04-12T20:13:56.629577', 'gensim': '4.1.2', 'python': '3.7.6 (default, Jan  8 2020, 13:42:34) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-20.6.0-x86_64-i386-64bit', 'event': 'train'}\n",
      "2023-04-12 20:13:56,630 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec(vocab=12133, vector_size=200, alpha=0.025)', 'datetime': '2023-04-12T20:13:56.630604', 'gensim': '4.1.2', 'python': '3.7.6 (default, Jan  8 2020, 13:42:34) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-20.6.0-x86_64-i386-64bit', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "# Train your own word vectors using gensim\n",
    "\n",
    "# gensim.models is the package for word2vec\n",
    "# check https://radimrehurek.com/gensim/models/word2vec.html\n",
    "# for detailed description\n",
    "\n",
    "from gensim.models import word2vec\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "# print out tracking information\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', \\\n",
    "                    level=logging.INFO)\n",
    "\n",
    "# min_count: words with total frequency lower than this are ignored\n",
    "# size: the dimension of word vector\n",
    "# window: context window, i.e. the maximum distance \n",
    "#         between the current and predicted word \n",
    "#         within a sentence (i.e. the length of ngrams)\n",
    "# workers: # of parallel threads in training\n",
    "# for other parameters, check https://radimrehurek.com/gensim/models/word2vec.html\n",
    "wv_model = word2vec.Word2Vec(sentences, \\\n",
    "            min_count=5,vector_size=200, \\\n",
    "            window=5, workers=4 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 words similar to word 'sound'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('metal', 0.7418868541717529),\n",
       " ('beats', 0.7396437525749207),\n",
       " ('production', 0.7243618965148926),\n",
       " ('vocals', 0.7202281951904297),\n",
       " ('sounds', 0.7157590389251709)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 words similar to word 'sound' but not relevant to 'film'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('rock', 0.7720798254013062),\n",
       " ('pop', 0.7414670586585999),\n",
       " ('beats', 0.727301836013794),\n",
       " ('songs', 0.7210579514503479),\n",
       " ('lyrics', 0.7147101759910583)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'movie' and 'film':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.918801"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'movie' and 'city':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.008146924"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word does not match with others in the list of ['sound', 'music', 'graphics', 'actor', 'book']:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'book'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word vector for 'movie':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.27586794e+00, -1.55183077e-01, -8.59031141e-01, -2.40213901e-01,\n",
       "       -1.13054955e+00,  4.17942643e-01,  6.60929918e-01,  4.77608591e-02,\n",
       "       -6.73678994e-01, -5.54073811e-01,  7.63609484e-02,  1.98808778e-02,\n",
       "        2.29297429e-01,  1.05217195e+00, -1.33877146e+00,  4.67828572e-01,\n",
       "        1.33434558e+00, -1.54373336e+00, -1.68750668e+00, -2.82854867e+00,\n",
       "        1.92898560e+00,  1.67880177e-01, -3.53898197e-01,  5.58144569e-01,\n",
       "       -5.66379607e-01,  7.45268464e-01, -1.72002172e+00, -6.55121148e-01,\n",
       "        1.52349007e+00,  4.76395994e-01,  8.02174568e-01,  5.40708244e-01,\n",
       "       -3.88299286e-01, -5.14961779e-02, -1.37928200e+00,  1.69840622e+00,\n",
       "       -2.23905230e+00, -8.48303214e-02, -1.46513450e+00,  4.11255807e-01,\n",
       "        1.32939541e+00, -1.60527551e+00,  4.35094446e-01, -1.34233892e+00,\n",
       "       -1.94738835e-01,  1.57682508e-01, -3.29971492e-01,  2.64493167e-01,\n",
       "        2.07258016e-01,  1.80324465e-01, -7.95697212e-01, -1.29068136e+00,\n",
       "        6.08371496e-01,  1.16296247e-01, -1.64115679e+00, -1.02694106e+00,\n",
       "       -1.11180437e+00, -1.66738939e+00, -1.28554690e+00,  1.30484045e+00,\n",
       "        1.44158781e+00,  7.92092562e-01,  1.35939747e-01, -6.32175624e-01,\n",
       "       -2.70380878e+00, -5.98043025e-01,  2.62853742e-01,  4.21702176e-01,\n",
       "       -1.16193318e+00,  4.39458191e-01,  1.28361389e-01,  5.50113618e-01,\n",
       "        1.86435238e-01,  4.63423610e-01,  5.47105037e-02, -1.00352716e+00,\n",
       "        1.88178360e+00, -1.00025997e-01, -1.48515511e+00, -8.54292884e-02,\n",
       "       -1.53191909e-01,  1.19667165e-01, -7.81285942e-01, -2.37496346e-01,\n",
       "       -7.51278922e-02,  4.36546922e-01,  1.39707363e+00,  9.20396984e-01,\n",
       "       -1.10813320e+00,  2.39081025e-01,  2.06064150e-01,  3.74416590e-01,\n",
       "        8.51087689e-01,  1.01330228e-01,  4.08203229e-02, -4.41797704e-01,\n",
       "       -2.03949809e+00, -6.16802216e-01,  3.06075096e-01,  2.03031611e+00,\n",
       "        8.12488317e-01,  1.70779109e-01,  6.91801131e-01, -1.34091365e+00,\n",
       "        1.56829774e+00,  2.09911078e-01,  3.48673202e-02, -9.37241971e-01,\n",
       "       -1.70190468e-01,  2.42191911e+00,  2.65068620e-01, -1.02598679e+00,\n",
       "        9.78646398e-01, -1.17883408e+00,  1.25826836e+00,  5.50553203e-01,\n",
       "        2.79085135e+00, -2.51727343e-01,  1.65890343e-02,  3.50417078e-01,\n",
       "        4.86127675e-01,  8.08006823e-01,  1.23090431e-01,  7.53800929e-01,\n",
       "        9.64164287e-02,  7.16134012e-01,  1.28234994e+00,  7.68398643e-02,\n",
       "       -3.93847466e-01, -3.43148619e-01,  1.43251371e+00, -1.42959714e+00,\n",
       "        6.67635620e-01, -1.41943169e+00,  3.10802817e-01, -2.24312472e+00,\n",
       "        4.01876390e-01,  1.31052899e+00,  4.54438776e-01,  2.02492565e-01,\n",
       "       -9.34926048e-03,  1.05007541e+00,  3.70025903e-01, -6.15345657e-01,\n",
       "       -4.33257431e-01, -3.52258682e-01, -3.46958101e-01,  8.56842458e-01,\n",
       "        1.67838871e-01, -1.55784512e+00,  3.51826549e-02, -7.22363651e-01,\n",
       "        1.87081754e+00, -3.94759983e-01,  8.77177238e-01,  1.15747035e+00,\n",
       "        1.22491169e+00,  2.00791931e+00, -5.12818158e-01,  5.48693836e-01,\n",
       "        4.40501779e-01, -5.25913000e-01, -5.34266472e-01,  2.57514858e+00,\n",
       "        6.29939795e-01,  1.37090361e+00, -7.26770997e-01,  8.05666029e-01,\n",
       "       -1.10311873e-01, -1.02607286e+00, -9.26617444e-01,  2.52587229e-01,\n",
       "       -9.24945295e-01, -1.63435185e+00, -1.07188725e+00, -1.28130063e-01,\n",
       "        8.73328128e-04,  1.88557136e+00,  9.01778281e-01,  6.27431512e-01,\n",
       "       -5.49355268e-01, -9.00682390e-01,  7.92493045e-01,  9.70755756e-01,\n",
       "        1.38648733e-01, -3.19072381e-02,  9.54972282e-02,  6.02498531e-01,\n",
       "        1.24890840e+00,  2.93272972e-01, -3.84596456e-03, -6.37133062e-01,\n",
       "       -1.15558684e+00, -7.66796291e-01,  1.49034441e+00,  1.30384374e+00,\n",
       "       -2.49156103e-01,  3.43825370e-02, -1.75833738e+00,  9.80621874e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test word2vec model\n",
    "\n",
    "print(\"Top 5 words similar to word 'sound'\")\n",
    "wv_model.wv.most_similar('sound', topn=5)\n",
    "\n",
    "print(\"Top 5 words similar to word 'sound' but not relevant to 'film'\")\n",
    "wv_model.wv.most_similar(positive=['sound','music'], \\\n",
    "                         negative=['film'], topn=5)\n",
    "\n",
    "print(\"Similarity between 'movie' and 'film':\")\n",
    "wv_model.wv.similarity('movie','film') \n",
    "\n",
    "print(\"Similarity between 'movie' and 'city':\")\n",
    "wv_model.wv.similarity('movie','city') \n",
    "\n",
    "print(\"Word does not match with others in the list of \\\n",
    "['sound', 'music', 'graphics', 'actor', 'book']:\")\n",
    "wv_model.wv.doesnt_match([\"sound\", \"music\", \\\n",
    "                          \"graphics\", \"actor\", \"book\"])\n",
    "\n",
    "print(\"Word vector for 'movie':\")\n",
    "wv_model.wv['movie']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Pretrained Word Vectors\n",
    "- Google published pre-trained 300-dimensional vectors for 3 million words and phrases that were trained on Google News dataset (about 100 billion words)(https://code.google.com/archive/p/word2vec/)\n",
    "- GloVe (Global Vectors for Word Representation): Pretained word vectors from different data sources provided by Standford https://nlp.stanford.edu/projects/glove/\n",
    "- FastText by Facebook https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-12 20:14:46,630 : INFO : loading projection weights from GoogleNews-vectors-negative300.bin\n",
      "2023-04-12 20:15:14,863 : INFO : KeyedVectors lifecycle event {'msg': 'loaded (3000000, 300) matrix of type float32 from GoogleNews-vectors-negative300.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2023-04-12T20:15:14.863445', 'gensim': '4.1.2', 'python': '3.7.6 (default, Jan  8 2020, 13:42:34) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-20.6.0-x86_64-i386-64bit', 'event': 'load_word2vec_format'}\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1.2: Use pretrained word vectors\n",
    "\n",
    "# download the bin file for pretrained word vectors\n",
    "# from above links, e.g. https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing\n",
    "# Warning: the bin file is very big (over 2G)\n",
    "# You need a powerful machine to load it\n",
    "\n",
    "import gensim\n",
    "\n",
    "model = gensim.models.KeyedVectors.\\\n",
    "load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.4827326238155365),\n",
       " ('queens', 0.466781347990036),\n",
       " ('kumaris', 0.4653734564781189),\n",
       " ('kings', 0.4558638632297516),\n",
       " ('womens', 0.422832190990448),\n",
       " ('princes', 0.4176960587501526),\n",
       " ('Al_Anqari', 0.41725507378578186),\n",
       " ('concubines', 0.4011078476905823),\n",
       " ('monarch', 0.3962482810020447),\n",
       " ('monarchy', 0.39430150389671326)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['women','king'], \\\n",
    "                      negative='man')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'artificial intelligence can take over the world.',\n",
       "  'score': 0.3182401657104492,\n",
       "  'token': 2064,\n",
       "  'token_str': 'can'},\n",
       " {'sequence': 'artificial intelligence will take over the world.',\n",
       "  'score': 0.18299691379070282,\n",
       "  'token': 2097,\n",
       "  'token_str': 'will'},\n",
       " {'sequence': 'artificial intelligence to take over the world.',\n",
       "  'score': 0.05600118637084961,\n",
       "  'token': 2000,\n",
       "  'token_str': 'to'},\n",
       " {'sequence': 'artificial intelligences take over the world.',\n",
       "  'score': 0.04519489035010338,\n",
       "  'token': 2015,\n",
       "  'token_str': '##s'},\n",
       " {'sequence': 'artificial intelligence would take over the world.',\n",
       "  'score': 0.04515336453914642,\n",
       "  'token': 2052,\n",
       "  'token_str': 'would'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install transformers\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
    "unmasker(\"Artificial Intelligence [MASK] take over the world.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "# Load the BERT tokenizer.\n",
    "from transformers import BertTokenizer, BertModel\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'here', 'is', 'the', 'sentence', 'i', 'want', 'em', '##bed', '##ding', '##s', 'for', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "text = \"Here is the sentence I want embeddings for.\"\n",
    "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "# Tokenize our sentence with the BERT tokenizer.\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "# Print out the tokens.\n",
    "print (tokenized_text)\n",
    "#The original word has been split into smaller subwords and characters. \n",
    "#The two hash signs preceding some of these subwords are just our tokenizer’s way to denote that this subword \n",
    "# or character is part of a larger word and preceded by another subword.\n",
    "# this way some contextual meaning of the original word will be retained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['knight',\n",
       " 'lap',\n",
       " 'survey',\n",
       " 'ma',\n",
       " '##ow',\n",
       " 'noise',\n",
       " 'billy',\n",
       " '##ium',\n",
       " 'shooting',\n",
       " 'guide',\n",
       " 'bedroom',\n",
       " 'priest',\n",
       " 'resistance',\n",
       " 'motor',\n",
       " 'homes',\n",
       " 'sounded',\n",
       " 'giant',\n",
       " '##mer',\n",
       " '150',\n",
       " 'scenes']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check out contents of BERT’s vocabulary\n",
    "list(tokenizer.vocab.keys())[5000:5020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:  This is a little longer and more detailed than the first two books in the series. However, I have enjoyed each new aspect of the exciting fantasy universe.\n",
      "Tokenized:  ['this', 'is', 'a', 'little', 'longer', 'and', 'more', 'detailed', 'than', 'the', 'first', 'two', 'books', 'in', 'the', 'series', '.', 'however', ',', 'i', 'have', 'enjoyed', 'each', 'new', 'aspect', 'of', 'the', 'exciting', 'fantasy', 'universe', '.']\n",
      "Token IDs:  [2023, 2003, 1037, 2210, 2936, 1998, 2062, 6851, 2084, 1996, 2034, 2048, 2808, 1999, 1996, 2186, 1012, 2174, 1010, 1045, 2031, 5632, 2169, 2047, 7814, 1997, 1996, 10990, 5913, 5304, 1012]\n"
     ]
    }
   ],
   "source": [
    "# use our data\n",
    "data=data.iloc[:100]\n",
    "sentences=data[\"text\"].values\n",
    "# Print the original sentence.\n",
    "print(' Original: ', sentences[0])\n",
    "\n",
    "# Print the sentence split into tokens.\n",
    "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
    "\n",
    "# Print the sentence mapped to token ids.\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "input_ids = []\n",
    "#attention_masks = []\n",
    "max_len =50\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = max_len,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        #return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "\n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    #attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "#attention_masks = torch.cat(attention_masks, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                    #output_attentions = False, # Whether the model returns attentions weights.\n",
    "                                    output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    "                                  )\n",
    "\n",
    "## Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "bert_model.eval()\n",
    "with torch.no_grad():\n",
    "\n",
    "    outputs = bert_model(input_ids)  \n",
    "    #the third item will be the hidden states from all layers.\n",
    "    hidden_states = outputs[2]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
      "Number of batches: 100\n",
      "Number of tokens: 50\n",
      "Number of hidden units: 768\n"
     ]
    }
   ],
   "source": [
    "print (\"Number of layers:\", len(hidden_states), \"  (initial embeddings + 12 BERT layers)\")\n",
    "layer_i = 0\n",
    "print (\"Number of batches:\", len(hidden_states[layer_i]))\n",
    "#The second dimension, the batch size, is used when submitting multiple sentences to the model at once\n",
    "batch_i = 0\n",
    "print (\"Number of tokens:\", len(hidden_states[layer_i][batch_i]))\n",
    "token_i = 0\n",
    "print (\"Number of hidden units:\", len(hidden_states[layer_i][batch_i][token_i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which vector works best as a contextualized embedding?\n",
    "- check out the authors: http://jalammar.github.io/illustrated-bert/\n",
    "<img src=\"bert layer.webp\" width =\"100%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 100, 50, 768])\n",
      "torch.Size([100, 50, 4, 768])\n",
      "torch.Size([100, 50, 768])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# get the last four layers\n",
    "token_embeddings = torch.stack(hidden_states[-4:], dim=0) \n",
    "print(token_embeddings.size())\n",
    "\n",
    "# permute axis\n",
    "token_embeddings = token_embeddings.permute(1,2,0,3)\n",
    "print(token_embeddings.size())\n",
    "\n",
    "# take the mean of the last 4 layers\n",
    "token_embeddings = token_embeddings.mean(axis=2)\n",
    "print(token_embeddings.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.1674e-01, -5.8241e-01,  4.6175e-02, -2.3729e-01, -5.1516e-01,\n",
       "        -1.8817e-01,  3.6290e-01,  2.7166e-01,  1.7761e-01, -8.4111e-01,\n",
       "        -1.9935e-01,  3.3611e-01,  2.5841e-01,  5.8463e-01,  7.7076e-01,\n",
       "         4.6129e-01, -1.8080e-01,  5.4306e-01,  3.8711e-01, -2.4097e-01,\n",
       "         3.7831e-01,  4.6802e-01,  8.4769e-01,  1.0741e-01,  1.6989e-01,\n",
       "        -2.8467e-01, -5.7680e-01, -1.5120e-01, -4.5195e-01,  7.4452e-02,\n",
       "        -4.1504e-01,  6.1159e-02,  3.2210e-02,  2.4624e-01,  6.4040e-01,\n",
       "        -2.9038e-01, -1.0278e-02, -2.5516e-01,  1.7088e-01,  3.1873e-02,\n",
       "        -4.9831e-01, -2.2552e-01,  4.3180e-01, -2.0207e-01,  3.9851e-01,\n",
       "        -3.9309e-01, -4.3959e+00,  1.9462e-01, -2.2815e-01,  2.0750e-01,\n",
       "         2.5107e-01, -3.9300e-01,  6.9135e-02,  2.4547e-01,  5.4014e-01,\n",
       "         8.6356e-01, -4.0756e-01, -4.7488e-01,  5.7281e-01,  1.6537e-04,\n",
       "         3.8241e-01,  2.6872e-01, -3.5878e-01, -1.2718e-01, -2.0059e-01,\n",
       "         1.6424e-01,  5.5433e-01, -1.9884e-01, -3.5233e-01,  1.1113e+00,\n",
       "         2.6240e-01,  4.8934e-01,  1.2434e-01,  5.2278e-01, -2.4245e-02,\n",
       "        -5.6428e-01, -4.3149e-01,  6.5885e-01, -6.5949e-01, -2.8488e-01,\n",
       "        -7.7719e-02,  2.2847e-01,  3.4881e-01, -3.0991e-01,  4.8294e-01,\n",
       "         5.2094e-01,  1.3239e-01, -2.6545e-01,  1.3792e-01,  1.4784e-01,\n",
       "        -1.2238e-01, -8.6928e-02, -5.9070e-01, -7.3859e-02,  8.4208e-01,\n",
       "        -3.9755e-01, -3.0791e-01, -4.0577e-01, -3.2322e-01,  8.1665e-01,\n",
       "         2.9392e-01,  8.4824e-01, -4.0191e-02, -8.0217e-01,  6.8353e-02,\n",
       "        -1.5561e-02, -3.5759e-01, -2.4667e-01, -1.2448e-01, -2.4065e-01,\n",
       "         2.1959e-01,  1.3923e-01,  3.7884e-02, -4.1337e-01,  1.7500e-01,\n",
       "        -1.5709e-01,  3.4158e-01,  1.1842e-01,  3.9512e-01, -4.2584e-01,\n",
       "        -1.4451e-01, -4.2437e-02,  2.8442e-02, -4.6329e-02,  3.4262e-01,\n",
       "         7.7092e-02,  2.2574e-01, -1.1013e+00,  2.8116e-01, -1.5778e-02,\n",
       "         2.1027e-01,  1.1082e-01, -5.8561e-01,  9.1119e-02, -6.2085e-02,\n",
       "         3.1870e-01,  1.0569e-01,  7.8224e-02, -6.8364e-01, -6.5966e-02,\n",
       "        -1.0196e+00, -7.7983e-01, -1.3280e+00,  1.6331e-01,  7.0949e-01,\n",
       "         7.0543e-01,  1.2083e-01,  7.0610e-01, -6.2007e-01,  4.9421e-02,\n",
       "        -2.8592e-01, -1.2609e-01, -6.9524e-01, -2.0708e-01, -2.9369e-01,\n",
       "        -6.7857e-01, -9.3166e-02,  1.2180e-01, -2.1345e-02,  3.7166e-01,\n",
       "        -6.0007e-02,  1.0312e-01,  4.7960e-04,  1.7923e-01,  4.7600e-01,\n",
       "         2.6793e-01,  5.6641e-01,  7.9208e-01,  2.6304e-01, -1.3294e-01,\n",
       "         6.6942e-02, -7.6227e-02,  2.7678e-01, -3.9513e-01,  5.7275e-02,\n",
       "         5.9705e-01,  5.5530e-02,  1.9647e-01, -3.2920e-02, -1.5051e-01,\n",
       "        -1.8463e-01,  6.4389e-02,  1.0789e-02,  3.4367e-01,  5.6598e-01,\n",
       "         2.8288e-01,  1.7805e-01,  4.0240e-01, -2.8867e-02,  6.2308e-01,\n",
       "        -7.7084e-01, -2.7658e-01,  1.3398e-01,  2.2497e-01,  5.5600e-01,\n",
       "        -8.0981e-02, -1.8217e-02, -2.2449e-02, -4.6839e-02, -4.6590e-01,\n",
       "         1.4388e-01,  4.5222e-01,  4.4107e-02,  4.3660e-01, -3.1193e-01,\n",
       "         2.0806e+00,  6.1539e-02,  6.8016e-03,  9.4153e-02, -4.5280e-02,\n",
       "        -8.5353e-01, -3.0534e-01, -5.3047e-01,  7.0621e-01, -2.6646e-01,\n",
       "        -4.1093e-02,  1.0881e-01, -4.8054e-01, -2.2758e-02, -8.8372e-02,\n",
       "        -1.9446e-01,  2.5115e-01, -3.4569e-01,  2.6481e-01, -4.4826e-01,\n",
       "        -1.5168e-01, -4.2276e-01, -6.7008e-02,  1.2732e-01, -1.7061e+00,\n",
       "         2.3185e-02, -6.8555e-01, -3.6767e-01,  6.8063e-02,  5.8538e-02,\n",
       "         4.0815e-01, -4.4172e-02, -7.2170e-01, -1.4472e-01, -2.4753e-01,\n",
       "        -2.4356e-01,  5.7488e-02,  6.6991e-01,  2.2269e-01,  4.4876e-02,\n",
       "         4.7304e-02, -2.6120e-01,  7.1398e-02,  6.1978e-02,  4.1831e-02,\n",
       "         3.7355e-01,  2.3525e-01,  1.7577e-01,  1.2959e-01,  3.6104e-01,\n",
       "         3.2902e-01, -2.6871e-01,  1.2853e-01, -6.4410e-01, -6.1203e-01,\n",
       "        -2.5680e-01, -3.1937e-01, -4.6457e-01,  2.1307e-01, -2.4077e-01,\n",
       "        -1.3629e-01,  5.3826e-01, -1.7773e-01,  4.6255e-01, -2.5471e-02,\n",
       "        -2.9789e-01, -2.9855e-01, -6.8101e-01,  2.4736e-01, -1.0669e-01,\n",
       "        -5.2193e-01,  2.0598e-01, -1.8340e-02,  1.4854e-01, -3.0242e-01,\n",
       "         5.8229e-01,  4.4244e-02, -5.9484e-01,  1.6363e-01, -3.7022e-05,\n",
       "        -4.1834e-01,  4.3708e-01, -5.3040e-01,  2.0828e-01,  6.0353e-02,\n",
       "        -5.2832e-01,  6.8439e-02, -7.3565e-01, -9.7323e-02,  2.5380e-01,\n",
       "        -3.1189e-01,  2.9857e-01,  3.5716e-01,  3.0574e-01, -2.4202e-01,\n",
       "        -1.2856e-01,  1.7648e-01,  4.7530e-02,  2.2718e-01, -1.5742e-01,\n",
       "         1.0190e-01, -4.6351e-01, -1.0159e-01, -6.5436e+00,  2.7761e-01,\n",
       "        -2.5390e-01, -1.6065e-01, -8.4697e-02,  4.8783e-01,  9.5073e-01,\n",
       "         2.8609e-01, -1.8638e-01,  2.0725e-01,  8.6598e-02,  4.0167e-01,\n",
       "         3.8852e-01, -5.6644e-01, -2.3602e-01, -1.9147e-01,  9.3460e-01,\n",
       "        -3.4325e-01,  2.0332e-01,  2.8043e-01, -2.7991e-01, -1.6388e-01,\n",
       "        -1.6414e-01,  1.6023e-02,  2.6993e-01, -9.6277e-03, -8.8753e-01,\n",
       "        -2.2775e-01,  2.1182e-01,  1.6237e-01,  1.8210e-01, -9.9728e-01,\n",
       "        -6.9462e-02, -1.5388e-01, -4.4140e-01, -2.8296e-01, -1.0347e-01,\n",
       "         7.9047e-02,  9.1998e-01, -5.7313e-02,  2.6998e-01, -1.1960e-01,\n",
       "        -4.8293e-01,  2.8153e-02,  5.9156e-01, -5.2177e-01,  5.1019e-01,\n",
       "         6.0029e-01,  2.0945e-01,  9.6613e-01,  4.0967e-02,  2.3941e-01,\n",
       "         1.2487e+00, -3.4248e-01,  2.2188e-01,  1.0250e-01,  1.0232e-01,\n",
       "         3.6594e-01, -4.3220e-01, -9.9285e-02,  1.0759e+00,  1.3798e-01,\n",
       "         3.2815e-02,  1.7804e-01,  4.4478e-01, -6.1109e-01,  7.0312e-01,\n",
       "        -6.5474e-01, -1.0161e+00, -4.1647e-03, -1.1553e+00,  1.8842e-01,\n",
       "        -4.0396e-01, -3.3793e+00,  2.2695e-01, -2.2415e-01, -2.5975e-01,\n",
       "         3.9460e-01, -4.0466e-01,  4.0743e-01, -6.2201e-01, -2.6534e-01,\n",
       "        -1.6050e-02,  4.3913e-01, -9.2362e-01,  3.6138e-01, -9.3469e-02,\n",
       "         2.9467e-01, -6.1127e-01,  1.1618e-01, -2.8022e-01,  9.0463e-02,\n",
       "         6.1554e-03,  9.1117e-02, -2.2541e-01,  6.4277e-01,  5.0465e-01,\n",
       "        -1.2755e-01,  5.0909e-01,  2.9349e-01,  1.2238e-01,  3.0729e-02,\n",
       "         1.6064e-01,  1.7079e-01, -5.5238e-02, -5.0800e-01, -2.8604e-01,\n",
       "        -9.8293e-02, -4.2392e-03,  1.1644e-01, -1.4208e-01, -3.9435e-01,\n",
       "        -3.8630e-01,  4.9813e-01,  7.4275e-01, -6.8219e-01, -2.1686e-01,\n",
       "         3.0953e-01,  3.8892e-01, -7.5905e-01, -3.8677e-02, -8.5610e-02,\n",
       "        -4.4929e-01, -2.3755e-01, -8.5067e-02, -1.4536e-01, -1.7305e-01,\n",
       "         4.3572e-01, -2.5971e-01, -2.2706e-01,  1.8178e-01,  7.5156e-02,\n",
       "        -1.2354e+00, -3.8476e-02, -3.4777e-01, -3.3714e-01, -3.4303e-01,\n",
       "        -6.0490e-01, -5.4858e-02,  4.8774e-01,  7.0761e-01,  1.9615e-01,\n",
       "        -4.9773e-01,  5.4514e-01, -7.5572e-01,  5.5636e-02, -1.7627e-02,\n",
       "        -2.4722e-02,  4.0634e-02,  3.9997e-01,  2.6073e-01,  2.2043e-01,\n",
       "         4.3057e-01, -3.4890e-01,  3.7281e-02,  5.0916e-01, -9.5202e-01,\n",
       "         5.4603e-01, -6.4651e-01, -1.5760e-01,  5.8655e-01, -2.1129e-01,\n",
       "        -2.0999e+00, -1.4026e-03,  5.1954e-01,  5.8836e-01,  5.4086e-01,\n",
       "         2.6716e-03, -7.4070e-02,  2.5329e-01, -1.3651e-01,  4.9498e-01,\n",
       "        -3.6194e-01, -1.7986e-01,  2.7265e-01,  5.9342e-02,  1.1412e-01,\n",
       "         3.9836e-03,  1.0881e-01, -1.7636e-01,  8.3392e-02, -2.8881e-01,\n",
       "        -1.0998e-01,  3.7503e-01,  3.4998e-01,  7.0962e-01,  2.8812e-01,\n",
       "        -2.2239e-01, -3.0763e-02,  2.8323e-01, -1.0333e-01,  5.4158e-01,\n",
       "         2.2385e-01, -6.3371e-01, -4.8591e-01, -1.0089e+00,  1.0880e+00,\n",
       "         5.6704e-01,  4.3626e-02,  3.6383e-01, -2.0883e-01,  6.0793e-01,\n",
       "        -3.0120e-01,  1.4469e-01,  9.3426e-01,  9.2225e-01,  7.0291e-03,\n",
       "         3.4239e-02,  2.3744e-01, -1.4372e-02, -2.1905e-01, -1.5742e-01,\n",
       "         3.2130e-01,  1.3797e-01,  3.0917e-02, -2.3620e-01, -4.9864e-01,\n",
       "        -2.4249e-01, -3.5059e-01, -2.3377e-01, -8.1488e-01,  7.5946e-02,\n",
       "         1.0669e+00,  1.0592e-01, -6.2600e-01, -2.6788e-01, -2.6539e-01,\n",
       "        -4.1650e-01, -9.5067e-02, -1.2507e-01, -4.5494e-01,  6.4973e-01,\n",
       "         4.1798e-01, -6.2821e-02, -7.9056e-01, -2.9883e-02,  9.3550e-02,\n",
       "        -3.3613e-01, -1.9968e-01,  4.7394e-01,  3.5558e-01, -1.9931e-01,\n",
       "         6.4410e-01, -5.6229e-01, -3.0320e-03,  6.0413e-01, -3.1644e-01,\n",
       "         7.3956e-02, -2.3796e-01, -1.6798e-01,  5.5720e-01, -3.0203e-01,\n",
       "        -1.4711e-01, -6.4964e-01,  4.9631e-01, -1.7784e-02,  1.9244e-01,\n",
       "         1.2528e-02, -3.4845e-01, -2.1717e-01, -4.8768e-01,  7.8929e-02,\n",
       "        -9.9126e-02, -1.5081e-01,  4.8844e-01, -1.1773e-01,  3.7841e-01,\n",
       "         7.3059e-01,  2.2871e-01, -6.0538e-02, -3.2224e-01,  1.4177e-01,\n",
       "         4.1905e-01,  8.1590e-02, -6.2040e-01,  6.1706e-01,  3.0352e-01,\n",
       "        -2.9337e-01,  4.6227e-02, -7.4995e-01,  6.4088e-02,  1.8523e-02,\n",
       "         2.5785e-01,  3.6435e-02,  2.7939e-01, -4.7244e-02, -1.4837e-01,\n",
       "        -4.3425e-01, -1.7650e-01,  6.9357e-01, -5.6927e-01,  9.1171e-01,\n",
       "        -2.5177e-01, -2.5150e-01,  6.5689e-01,  7.0479e-01,  3.0306e-01,\n",
       "        -6.9310e-01, -8.0301e-01,  2.5094e-01, -3.9435e-01,  3.8376e-01,\n",
       "         4.9443e-01, -6.8599e-01, -3.9733e-01,  2.1738e-01, -8.5135e-02,\n",
       "         3.2226e-01,  9.1791e-02,  3.7680e-01,  1.4275e-01,  1.9376e-01,\n",
       "        -7.2868e-02,  1.0137e+00, -3.6094e-01, -3.2843e-01, -4.7212e-01,\n",
       "        -4.7861e-01, -1.8302e-01, -1.2255e-04, -4.2985e-01, -1.2398e-01,\n",
       "         4.6820e-01, -2.4856e-01, -1.3760e-01,  6.5729e-01,  5.6273e-03,\n",
       "        -4.2098e-01,  1.6346e-01,  7.7030e-01, -2.8659e-01, -5.8063e-01,\n",
       "         1.0748e-01,  3.2548e-01, -6.4719e-02, -1.9482e-01, -4.1841e-01,\n",
       "        -7.6163e-01, -5.1279e-01,  9.1793e-01, -4.4523e-01,  7.1406e-02,\n",
       "         3.5033e-01, -4.7681e-01, -1.1784e-01,  6.8566e-02, -2.2953e-01,\n",
       "         3.1152e-01,  2.9084e-01, -2.8166e-01,  3.5153e-02,  5.5383e-01,\n",
       "         1.0160e+00,  6.2193e-01,  1.3250e-01,  6.0047e-01,  4.6697e-01,\n",
       "        -7.6861e-01, -1.4042e-01, -2.0490e-01, -1.1983e-01,  3.7807e-02,\n",
       "        -2.5424e-02, -4.4065e-02,  1.8069e-01,  1.5926e-01,  1.3611e-01,\n",
       "         7.3470e-01, -3.0914e-01, -5.4038e-01,  6.8040e-01,  2.0963e-01,\n",
       "        -2.0104e-01,  1.4587e-01, -4.4374e-01,  4.4258e-01, -9.4547e-03,\n",
       "        -4.6953e-02, -3.8309e-01, -9.7249e-02, -1.8281e-01, -8.0588e-01,\n",
       "        -3.9593e-01, -1.9447e-01,  7.8777e-01,  4.3225e-01,  2.5621e-02,\n",
       "        -8.6588e-02, -1.1657e-01,  1.2488e-01,  4.2359e-01, -1.0062e-01,\n",
       "        -4.5902e-01,  7.1786e-01,  2.1758e-01, -2.7604e-01, -2.5328e-01,\n",
       "        -3.0193e-01,  4.6264e-01, -2.6710e-01,  6.6475e-01, -1.7954e-01,\n",
       "         6.5488e-01,  3.6956e-01, -2.4965e-01,  3.9864e-01, -6.7487e-01,\n",
       "         4.1960e-01, -4.2689e-01,  2.0901e-01,  3.8223e-02,  4.8119e-01,\n",
       "         1.5156e-01,  6.7575e-01,  1.1677e-01, -8.1947e-02, -1.0539e-01,\n",
       "         4.6403e-01,  1.5562e-01, -7.3558e-01, -6.8996e-02, -2.7774e-01,\n",
       "         1.4826e-01,  3.3956e-01, -2.4024e-01, -5.4097e-01, -3.2693e-01,\n",
       "        -7.6715e-01, -7.8082e-01, -2.4397e-01,  1.7152e-01,  1.4835e-01,\n",
       "        -9.5983e-02,  2.2353e-01, -2.6012e-01, -2.2674e-01,  4.8688e-01,\n",
       "         8.1932e-02, -1.0242e-01,  1.0640e-01, -1.7737e-01,  2.1931e-01,\n",
       "        -1.3066e-01,  1.3798e-01, -1.4375e+00,  1.9847e-01, -2.1998e-01,\n",
       "         2.1900e-01, -6.0209e-01, -7.3216e-01,  9.5402e-02, -5.5060e-01,\n",
       "        -7.7929e-01, -6.4954e-01,  2.3006e-01, -6.5008e-01,  3.2119e-01,\n",
       "        -1.8126e-02,  2.9258e-01, -3.1719e-01])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings[0,0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. How to use word vectors in classification?\n",
    "\n",
    "`Convolutional Neural Network`\n",
    "<img src=\"CNN.png\" width =\"100%\">\n",
    "\n",
    "`Recurrent Neural Network`\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/graviraja/100-Days-of-NLP/master/assets/images/applications/sentiment/simple.gif\" width = \"90%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.kdnuggets.com/images/cartoon-machine-learning-vacation.jpg\" width='60%'>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
