{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ac5346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import spacy\n",
    "import nltk\n",
    "from string import punctuation\n",
    "import csv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "from typing import List\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction import text \n",
    "# from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import networkx as nx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca5153e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# set the file path for scraped jobs \n",
    "file_path = 'swe_us_ft_4wks_041923.csv'\n",
    "\n",
    "# read the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6549ed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# find type of work (remote, hybrid, or inperson) and state if applicable\n",
    "work_week = []\n",
    "\n",
    "for loc in df['location']:\n",
    "    state_match = re.findall(r'(?<![A-Z])[A-Z]{2}(?![A-Z])', loc)\n",
    "    \n",
    "    if 'hybrid' in loc.lower():\n",
    "        job_type = 'hybrid'\n",
    "    elif 'remote' in loc.lower():\n",
    "        job_type = 'remote'\n",
    "        state_match = ['remote']\n",
    "    else:\n",
    "        job_type = 'in-person'\n",
    "        \n",
    "    work_week.append((job_type,state_match[0]))\n",
    "    \n",
    "df['type'], df['location'] = zip(*work_week)\n",
    "\n",
    "# reformat salary column to avg annual\n",
    "hrs_per_year = 40*52\n",
    "salary = []\n",
    "\n",
    "for s in df['salary']:\n",
    "    if type(s) == str and any(ch.isdigit() for ch in s):\n",
    "        # Define the regular expression pattern to match salary ranges\n",
    "        pattern = r'\\$([\\d,]+)'\n",
    "        # Use regular expression to extract the lower and upper bound of the salary range\n",
    "        match = re.findall(pattern, s.replace(',',''))\n",
    "        avg_salary = sum([int(m) for m in match])/len(match)\n",
    "        # adjust to annual\n",
    "        if 'an hour' in s:\n",
    "            avg_salary = avg_salary*hrs_per_year\n",
    "        if 'month' in s:\n",
    "            avg_salary = avg_salary*12\n",
    "        if 'Estimated' in s:\n",
    "            avg_salary = avg_salary*1000\n",
    "        \n",
    "        s = avg_salary\n",
    "        \n",
    "    salary.append(s)\n",
    "\n",
    "df['salary'] = salary\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c493f2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_a_doc(doc, nlp, lemmatized=True, remove_stopword=True, remove_punct=True): \n",
    "    clean_tokens = []\n",
    "    # load current doc into spacy nlp model and split sentences by newline chars\n",
    "    sentences = doc.split(\"\\n\")\n",
    "    for sentence in sentences:\n",
    "        doc = nlp(sentence)\n",
    "        # clean either lemmatized unigrams or unmodified doc tokens\n",
    "        if lemmatized:\n",
    "            clean_tokens += [token.lemma_.lower() for token in doc            # using spacy nlp params, skip token if:\n",
    "                            if (not remove_stopword or not token.is_stop)     # it is a stopword and remove_stopwords = True\n",
    "                            and (not remove_punct or not token.is_punct)      # it is punctuation and remove_punct = True\n",
    "                            and not token.lemma_.isspace()]                   # it is whitespace                               \n",
    "        else:\n",
    "            clean_tokens += [token.text.lower() for token in doc \n",
    "                            if (not remove_stopword or not token.is_stop) \n",
    "                            and (not remove_punct or not token.is_punct) \n",
    "                            and not token.text.isspace()]\n",
    "    return clean_tokens\n",
    "\n",
    "def tokenize(docs, lemmatized=True, remove_stopword=True, remove_punct=True):\n",
    "    # load in spacy NLP model and disable unused pipelines to reduce processing time/memory space\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "    nlp.add_pipe(\"sentencizer\")\n",
    "    # tokenize each doc in the corpus using specified params for lemmatization and removal conditions\n",
    "    tokens = [tokenize_a_doc(doc, nlp, lemmatized, remove_stopword, remove_punct) for doc in docs]\n",
    "     # drop terms that appear in at least 80 docs, or 5 docs or less, and additional stop words\n",
    "    add_stop = ['e.g.']\n",
    "    tokens = drop_top_and_bottom(tokens, 80, 5, add_stop)  \n",
    "    return tokens\n",
    "\n",
    "\n",
    "def drop_top_and_bottom(tokens, top, bot, add_stop):\n",
    "    docs_tokens = {idx:nltk.FreqDist(tokens) for idx,tokens in enumerate(tokens)}\n",
    "    dtm=pd.DataFrame.from_dict(docs_tokens, orient=\"index\").fillna(0).sort_index(axis = 0)\n",
    "    docs_per_term = dtm.astype(bool).sum(axis=0)\n",
    "\n",
    "    tokens = [[token for token in t if (docs_per_term[token] > bot and docs_per_term[token] < top \n",
    "                                        and token not in add_stop)] for t in tokens]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "tokens = tokenize(df['descriptions'], lemmatized=True, remove_stopword=True, remove_punct=True)\n",
    "df['cleaned_desc'] = pd.Series(tokens).apply(lambda x: \" \".join(x))\n",
    "\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f242264d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# find k most popular n-grams\n",
    "def most_popular_ngrams(corpus, n, top_k):\n",
    "    # Create a TfidfVectorizer with n-grams\n",
    "    params = {\n",
    "        'ngram_range':n,\n",
    "        'smooth_idf':True\n",
    "    }\n",
    "    vectorizer = TfidfVectorizer(**params)\n",
    "    \n",
    "    # get term-document matrix\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "    smoothed_tfidf = tfidf_matrix.toarray()\n",
    "    \n",
    "    # Calculate the average score for each n-gram\n",
    "    avg_score = np.mean(smoothed_tfidf, axis=0)\n",
    "    \n",
    "    # Get the top-k n-grams based on their average TF-IDF scores\n",
    "    ngrams = vectorizer.get_feature_names_out()\n",
    "    top_ngrams = sorted(zip(ngrams, avg_score), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "    top_ngrams_list = [ngram for ngram, score in top_ngrams]\n",
    "    \n",
    "    return top_ngrams_list, pd.DataFrame(smoothed_tfidf, columns = ngrams)\n",
    "\n",
    "# get top phrases \n",
    "n = 3\n",
    "top_k = 500\n",
    "\n",
    "popular_phrases, smoothed_tfidf_ngrams = most_popular_ngrams(df['cleaned_desc'], (n,n), top_k)\n",
    "print(f'Top {top_k} phrases using full cleaned description:\\n \\n{popular_phrases}\\n')\n",
    "\n",
    "n = 1\n",
    "popular_phrases, smoothed_tfidf_ngrams = most_popular_ngrams(df['cleaned_desc'], (n,n), top_k)\n",
    "print(f'Top {top_k} phrases using full cleaned description:\\n \\n{popular_phrases}\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02f10f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show semantic network\n",
    "\n",
    "\n",
    "n = 1\n",
    "top_k = 20\n",
    "popular_phrases, smoothed_tfidf_ngrams = most_popular_ngrams(df['cleaned_desc'], (n,n), top_k)\n",
    "print(f'Top {top_k} phrases using full cleaned description:\\n \\n{popular_phrases}\\n')\n",
    "\n",
    "# Compute cosine similarity between smoothed TF-IDF vectors\n",
    "similarity_matrix = cosine_similarity(smoothed_tfidf_ngrams)\n",
    "\n",
    "# Create a semantic network as a graph\n",
    "G = nx.Graph()\n",
    "G = nx.from_numpy_matrix(similarity_matrix, create_using=nx.Graph()) # creating from adjacency matrix \n",
    "node_name = dict(zip(range(0,len(list(G.nodes()))), smoothed_tfidf_ngrams.columns))  \n",
    "G = nx.relabel_nodes(G, node_name)\n",
    "\n",
    "# Take a look\n",
    "plt.figure(figsize=(20,20), dpi=100)\n",
    "\n",
    "# node size proportional to eigenvector centrality of words\n",
    "node_size= [x*1000 for x in nx.eigenvector_centrality(G).values()]\n",
    "\n",
    "# choose a layout function\n",
    "pos=nx.kamada_kawai_layout(G)\n",
    "\n",
    "# edge color\n",
    "edge_color='0.8'\n",
    "\n",
    "nx.drawing.nx_pylab.draw_networkx(G,\n",
    "                                  node_size=node_size, \n",
    "                                  node_color = \"pink\",\n",
    "                                  pos=pos,\n",
    "                                  edge_color=edge_color,\n",
    "                                  with_labels=True,\n",
    "                                 font_size=12)\n",
    "plt.title('none')\n",
    "plt.axis(\"off\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc7ce97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show semantic network\n",
    "\n",
    "\n",
    "n = 3\n",
    "top_k = 20\n",
    "popular_phrases, smoothed_tfidf_ngrams = most_popular_ngrams(df['cleaned_desc'], (n,n), top_k)\n",
    "print(f'Top {top_k} phrases using full cleaned description:\\n \\n{popular_phrases}\\n')\n",
    "\n",
    "# Compute cosine similarity between smoothed TF-IDF vectors\n",
    "similarity_matrix = cosine_similarity(smoothed_tfidf_ngrams)\n",
    "\n",
    "# Create a semantic network as a graph\n",
    "G = nx.Graph()\n",
    "G = nx.from_numpy_matrix(similarity_matrix, create_using=nx.Graph()) # creating from adjacency matrix \n",
    "node_name = dict(zip(range(0,len(list(G.nodes()))), smoothed_tfidf_ngrams.columns))  \n",
    "G = nx.relabel_nodes(G, node_name)\n",
    "\n",
    "# Take a look\n",
    "plt.figure(figsize=(20,20), dpi=100)\n",
    "\n",
    "# node size proportional to eigenvector centrality of words\n",
    "node_size= [x*1000 for x in nx.eigenvector_centrality(G).values()]\n",
    "\n",
    "# choose a layout function\n",
    "pos=nx.kamada_kawai_layout(G)\n",
    "\n",
    "# edge color\n",
    "edge_color='0.8'\n",
    "\n",
    "nx.drawing.nx_pylab.draw_networkx(G,\n",
    "                                  node_size=node_size, \n",
    "                                  node_color = \"pink\",\n",
    "                                  pos=pos,\n",
    "                                  edge_color=edge_color,\n",
    "                                  with_labels=True,\n",
    "                                 font_size=12)\n",
    "plt.title('none')\n",
    "plt.axis(\"off\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66639631",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def most_popular_skills(phrases, skills, n):\n",
    "    vectorizer = CountVectorizer().fit_transform(phrases + skills)\n",
    "    similarity_matrix = cosine_similarity(vectorizer)[:len(phrases), len(phrases):]\n",
    "    \n",
    "    average_similarities = np.mean(similarity_matrix, axis=0)\n",
    "    sorted_indices = np.argsort(average_similarities)[::-1]\n",
    "    \n",
    "    popular_elements = [skills[i] for i in sorted_indices[:n]]\n",
    "    return popular_elements\n",
    "\n",
    "\n",
    "tech_skills = pd.read_csv('Hot_Technologies_Software_Developer_ONET.csv')\n",
    "tech_skills = tech_skills.loc[tech_skills['In Demand'] == 'Yes']\n",
    "tech_skill_names = tech_skills['Technology Skill'].apply(lambda x: x.lower().replace('software','')).to_list()\n",
    "\n",
    "soft_skills = pd.read_csv('soft_skills.csv')\n",
    "soft_skill_names = soft_skills['skill_name']\n",
    "\n",
    "n = 20\n",
    "most_popular = most_popular_skills(popular_phrases, tech_skill_names, n)\n",
    "print(f'\\nTop {n} popular technical skills based on top n-grams: \\n{most_popular}')\n",
    "\n",
    "\n",
    "most_popular = most_popular_skills(popular_phrases, list(soft_skill_names), n)\n",
    "print(f'\\nTop {n} popular soft skills based on top n-grams: \\n{most_popular}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac57ee0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "states = 'AL, AK, AZ, AR, CA, CO, CT, DE, DC, FL, GA, HI, ID, IL, IN, IA, KS, KY, LA, ME, MD, \\\n",
    "MA, MI, MN, MS, MO, MT, NE, NV, NH, NJ, NM, NY, NC, ND, OH, OK, OR, PA, PR, RI, SC, \\\n",
    "SD, TN, TX, UT, VT, VA, WA, WV, WI, WY'.split(', ')\n",
    "\n",
    "annual_mean_wage = pd.read_csv('mean_annual_salary_swe.csv').loc[:51]\n",
    "annual_mean_wage['Area Name'] = pd.Series(states)\n",
    "\n",
    "# get US mean salary then convert state salary stats to dict with {state:avg_salary}\n",
    "us_avg_salary = round(annual_mean_wage['Annual mean wage'].mean(), 0)\n",
    "annual_mean_wage = dict(zip(annual_mean_wage.iloc[:,0],annual_mean_wage.iloc[:,1]))\n",
    "\n",
    "# calculate salary difference in comparison to location average\n",
    "salary_delta = []\n",
    "for i,location in enumerate(df['location']):\n",
    "    salary = df['salary'][i]\n",
    "    # if salary is known\n",
    "    if type(salary) != 'str':\n",
    "        # if remote, take difference from US average, else use state average\n",
    "        if location != 'remote':\n",
    "            salary_delta.append(salary-annual_mean_wage[location])\n",
    "        else:\n",
    "            salary_delta.append(salary-us_avg_salary)\n",
    "    else:\n",
    "        salary_delta.append(salary)\n",
    "\n",
    "df['salary_delta'] = salary_delta\n",
    "df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
