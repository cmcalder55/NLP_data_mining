{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW4: Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div class=\"alert alert-block alert-warning\">Each assignment needs to be completed independently. Never ever copy others' work (even with minor modification, e.g. changing variable names). Anti-Plagiarism software will be used to check all submissions. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Description\n",
    "\n",
    "In this assignment, we'll use what we learned in preprocessing module to compare ChatGPT-generated text with human-generated answers. A dataset with 200 questions and answers has been provided for you to use. The dataset can be found at https://huggingface.co/datasets/Hello-SimpleAI/HC3.\n",
    "\n",
    "\n",
    "Please follow the instruction below to do the assessment step by step and answer all analysis questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "import string\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import nltk\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>chatgpt_answer</th>\n",
       "      <th>human_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What happens if a parking ticket is lost / des...</td>\n",
       "      <td>If a parking ticket is lost or destroyed befor...</td>\n",
       "      <td>In my city you also get something by mail to t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>why the waves do n't interfere ? first , I 'm ...</td>\n",
       "      <td>Interference is the phenomenon that occurs whe...</td>\n",
       "      <td>They do actually . That 's why a microwave ove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Is it possible to influence a company's action...</td>\n",
       "      <td>Yes, it is possible to influence a company's a...</td>\n",
       "      <td>Yes and no. This really should be taught at ju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why do taxpayers front the bill for sports sta...</td>\n",
       "      <td>Sports stadiums are usually built with public ...</td>\n",
       "      <td>That 's the bargaining chip that team owners u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Why do clothing stores generally have a ton of...</td>\n",
       "      <td>There are a few reasons why clothing stores ma...</td>\n",
       "      <td>Your observation is almost certainly a matter ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What happens if a parking ticket is lost / des...   \n",
       "1  why the waves do n't interfere ? first , I 'm ...   \n",
       "2  Is it possible to influence a company's action...   \n",
       "3  Why do taxpayers front the bill for sports sta...   \n",
       "4  Why do clothing stores generally have a ton of...   \n",
       "\n",
       "                                      chatgpt_answer  \\\n",
       "0  If a parking ticket is lost or destroyed befor...   \n",
       "1  Interference is the phenomenon that occurs whe...   \n",
       "2  Yes, it is possible to influence a company's a...   \n",
       "3  Sports stadiums are usually built with public ...   \n",
       "4  There are a few reasons why clothing stores ma...   \n",
       "\n",
       "                                        human_answer  \n",
       "0  In my city you also get something by mail to t...  \n",
       "1  They do actually . That 's why a microwave ove...  \n",
       "2  Yes and no. This really should be taught at ju...  \n",
       "3  That 's the bargaining chip that team owners u...  \n",
       "4  Your observation is almost certainly a matter ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"qa.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Tokenize function\n",
    "\n",
    "Define a function `tokenize(docs, lemmatized = True, remove_stopword = True, remove_punct = True)`  as follows:\n",
    "   - Take three parameters: \n",
    "       - `docs`: a list of documents (e.g. questions)\n",
    "       - `lemmatized`: an optional boolean parameter to indicate if tokens are lemmatized. The default value is True (i.e. tokens are lemmatized).\n",
    "       - `remove_stopword`: an optional bookean parameter to remove stop words. The default value is True (i.e. remove stop words). \n",
    "   - Split each input document into unigrams and also clean up tokens as follows:\n",
    "       - if `lemmatized` is turned on, lemmatize all unigrams.\n",
    "       - if `remove_stopword` is set to True, remove all stop words.\n",
    "       - if `remove_punct` is set to True, remove all punctuation tokens.\n",
    "       - remove all empty tokens and lowercase all the tokens.\n",
    "   - Return the list of tokens obtained for each document after all the processing. \n",
    "   \n",
    "(Hint: you can use spacy package for this task. For reference, check https://spacy.io/api/token#attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_a_doc(doc, nlp, lemmatized=True, remove_stopword=True, remove_punct=True): \n",
    "    clean_tokens = []\n",
    "    # load current doc into spacy nlp model and split sentences by newline chars\n",
    "    sentences = doc.split(\"\\\\n\")\n",
    "    for sentence in sentences:\n",
    "        doc = nlp(sentence)\n",
    "    \n",
    "        # clean either lemmatized unigrams or unmodified doc tokens\n",
    "        if lemmatized:\n",
    "            clean_tokens += [token.lemma_.lower() for token in doc            # using spacy nlp params, skip token if:\n",
    "                            if (not remove_stopword or not token.is_stop)     # it is a stopword and remove_stopwords = True\n",
    "                            and (not remove_punct or not token.is_punct)      # it is punctuation and remove_punct = True\n",
    "                            and not token.lemma_.isspace()]                   # it is whitespace\n",
    "        else:\n",
    "            clean_tokens += [token.text.lower() for token in doc \n",
    "                            if (not remove_stopword or not token.is_stop) \n",
    "                            and (not remove_punct or not token.is_punct) \n",
    "                            and not token.text.isspace()]\n",
    "        \n",
    "    return clean_tokens\n",
    "\n",
    "def tokenize(docs, lemmatized=True, remove_stopword=True, remove_punct=True):\n",
    "    # load in spacy NLP model and disable unused pipelines to reduce processing time/memory space\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "    nlp.add_pipe(\"sentencizer\")\n",
    "    # tokenize each doc in the corpus using specified params for lemmatization and removal conditions\n",
    "    tokens = [tokenize_a_doc(doc, nlp, lemmatized, remove_stopword, remove_punct) for doc in docs]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your function with different parameter configuration and observe the differences in the resulting tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What happens if a parking ticket is lost / destroyed before the owner is aware of the ticket , and it goes unpaid ? I 've always been curious . Please explain like I'm five.\n",
      "\n",
      "1.lemmatized=True, remove_stopword=False, remove_punct = True:\n",
      " [['what', 'happen', 'if', 'a', 'parking', 'ticket', 'be', 'lose', 'destroy', 'before', 'the', 'owner', 'be', 'aware', 'of', 'the', 'ticket', 'and', 'it', 'go', 'unpaid', 'i', 've', 'always', 'be', 'curious', 'please', 'explain', 'like', 'i', 'be', 'five']]\n",
      "\n",
      "2.lemmatized=True, remove_stopword=True, remove_punct = True:\n",
      " [['happen', 'parking', 'ticket', 'lose', 'destroy', 'owner', 'aware', 'ticket', 'go', 'unpaid', 've', 'curious', 'explain', 'like']]\n",
      "\n",
      "3.lemmatized=False, remove_stopword=False, remove_punct = True:\n",
      " [['what', 'happens', 'if', 'a', 'parking', 'ticket', 'is', 'lost', 'destroyed', 'before', 'the', 'owner', 'is', 'aware', 'of', 'the', 'ticket', 'and', 'it', 'goes', 'unpaid', 'i', 've', 'always', 'been', 'curious', 'please', 'explain', 'like', 'i', \"'m\", 'five']]\n",
      "\n",
      "4.lemmatized=False, remove_stopword=False, remove_punct = False:\n",
      " [['what', 'happens', 'if', 'a', 'parking', 'ticket', 'is', 'lost', '/', 'destroyed', 'before', 'the', 'owner', 'is', 'aware', 'of', 'the', 'ticket', ',', 'and', 'it', 'goes', 'unpaid', '?', 'i', \"'\", 've', 'always', 'been', 'curious', '.', 'please', 'explain', 'like', 'i', \"'m\", 'five', '.']]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For simplicity, We will test on document\n",
    "\n",
    "print(data[\"question\"].iloc[0] + \"\\n\")\n",
    "\n",
    "print(f\"1.lemmatized=True, remove_stopword=False, remove_punct = True:\\n \\\n",
    "{tokenize(data['question'].iloc[0:1], lemmatized=True, remove_stopword=False, remove_punct = True)}\\n\")\n",
    "\n",
    "print(f\"2.lemmatized=True, remove_stopword=True, remove_punct = True:\\n \\\n",
    "{tokenize(data['question'].iloc[0:1], lemmatized=True, remove_stopword=True, remove_punct = True)}\\n\")\n",
    "\n",
    "print(f\"3.lemmatized=False, remove_stopword=False, remove_punct = True:\\n \\\n",
    "{tokenize(data['question'].iloc[0:1], lemmatized=False, remove_stopword=False, remove_punct = True)}\\n\")\n",
    "\n",
    "print(f\"4.lemmatized=False, remove_stopword=False, remove_punct = False:\\n \\\n",
    "{tokenize(data['question'].iloc[0:1], lemmatized=False, remove_stopword=False, remove_punct = False)}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Sentiment Analysis\n",
    "\n",
    "\n",
    "Let's check if there is any difference in sentiment between ChatGPT-generated and human-generated answers.\n",
    "\n",
    "\n",
    "Define a function `compute_sentiment(generated, reference, pos, neg )` as follows:\n",
    "- take four parameters:\n",
    "    - `gen_tokens` is the tokenized ChatGPT-generated answers by the `tokenize` function in Q1.\n",
    "    - `ref_tokens` is the tokenized human-generated answers by the `tokenize` function in Q1.\n",
    "    - `pos` (`neg`) is the lists of positive (negative) words, which can be find in Canvas preprocessing module.\n",
    "- for each ChatGPT-generated or human-generated answer, compute the sentiment as `(#pos - #neg )/(#pos + #neg)`, where `#pos`(`#neg`) is the number of positive (negative) words found in each answer. If an answer contains none of the positive or negative words, set the sentiment to 0.\n",
    "- return the sentiment of ChatGPT-generated and human-generated answers as two columns of DataFrame.\n",
    "\n",
    "\n",
    "Analysis: \n",
    "- Try different tokenization parameter configurations (lemmatized, remove_stopword, remove_punct), and observe how sentiment results change.\n",
    "- Do you think, in general, which tokenization configuration should be used? Why does this combination make the most senese?\n",
    "- Do you think, overall, ChatGPT-generated answers are more posive or negative than human-generated ones? Use data to support your conclusion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent(target, pos, neg):\n",
    "    p = sum(1 for word in target if word in pos)\n",
    "    n = sum(1 for word in target if word in neg)\n",
    "    if p + n != 0:\n",
    "        sentiment = (p - n) / (p + n)\n",
    "    else:\n",
    "        sentiment = 0\n",
    "    return sentiment\n",
    "\n",
    "def compute_sentiment(gen_tokens, ref_tokens, pos, neg):\n",
    "    \n",
    "    tokens = lambda token_list: [sent(sublist, pos, neg) for sublist in token_list]\n",
    "    result = pd.DataFrame({'gen_sentiment': tokens(gen_tokens), 'ref_sentiment': tokens(ref_tokens)})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_tokens = tokenize(data[\"chatgpt_answer\"], lemmatized=False, remove_stopword=False, remove_punct = False)\n",
    "ref_tokens = tokenize(data[\"human_answer\"], lemmatized=False, remove_stopword=False, remove_punct = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abounds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abundance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abundant</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0\n",
       "0         a+\n",
       "1     abound\n",
       "2    abounds\n",
       "3  abundance\n",
       "4   abundant"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2-faced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2-faces</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abnormal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abolish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abominable</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "0     2-faced\n",
       "1     2-faces\n",
       "2    abnormal\n",
       "3     abolish\n",
       "4  abominable"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = pd.read_csv(\"positive-words.txt\", header = None)\n",
    "pos.head()\n",
    "\n",
    "neg = pd.read_csv(\"negative-words.txt\", header = None)\n",
    "neg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gen_sentiment</th>\n",
       "      <th>ref_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.777778</td>\n",
       "      <td>0.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.600000</td>\n",
       "      <td>-0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gen_sentiment  ref_sentiment\n",
       "0       0.000000      -0.500000\n",
       "1      -0.777778       0.076923\n",
       "2       0.666667       0.200000\n",
       "3       1.000000       0.200000\n",
       "4       0.600000      -0.333333"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = compute_sentiment(gen_tokens, \n",
    "                           ref_tokens, \n",
    "                           pos[0].values,\n",
    "                           neg[0].values)\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1462586239453829"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(10279.5, 0.0011456573663914912)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import wilcoxon\n",
    "\n",
    "(result['gen_sentiment'] - result['ref_sentiment']).mean()\n",
    "\n",
    "res = wilcoxon(result['gen_sentiment'] - result['ref_sentiment'], alternative='greater')\n",
    "res.statistic, res.pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "#### Try different tokenization parameter configurations (lemmatized, remove_stopword, remove_punct), and observe how sentiment results change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gen_sentiment</th>\n",
       "      <th>ref_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.230769</td>\n",
       "      <td>-0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.777778</td>\n",
       "      <td>-0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.600000</td>\n",
       "      <td>-0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gen_sentiment  ref_sentiment\n",
       "0      -0.230769      -0.500000\n",
       "1      -0.777778      -0.066667\n",
       "2       0.666667       0.200000\n",
       "3       1.000000       0.200000\n",
       "4       0.600000      -0.333333"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.13499355278093766"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(10576.5, 0.002139873867261238)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_tokens = tokenize(data[\"chatgpt_answer\"], lemmatized=True, remove_stopword=False, remove_punct = True)\n",
    "ref_tokens = tokenize(data[\"human_answer\"], lemmatized=True, remove_stopword=False, remove_punct = True)\n",
    "result = compute_sentiment(gen_tokens, \n",
    "                           ref_tokens, \n",
    "                           pos[0].values,\n",
    "                           neg[0].values)\n",
    "result.head()\n",
    "\n",
    "(result['gen_sentiment'] - result['ref_sentiment']).mean()\n",
    "\n",
    "res = wilcoxon(result['gen_sentiment'] - result['ref_sentiment'], alternative='greater')\n",
    "res.statistic, res.pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gen_sentiment</th>\n",
       "      <th>ref_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.230769</td>\n",
       "      <td>-0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.777778</td>\n",
       "      <td>-0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.600000</td>\n",
       "      <td>-0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gen_sentiment  ref_sentiment\n",
       "0      -0.230769      -0.500000\n",
       "1      -0.777778      -0.142857\n",
       "2       0.666667       0.111111\n",
       "3       1.000000       0.200000\n",
       "4       0.600000      -0.333333"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.14764114639510492"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(10680.0, 0.0008091725092345695)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_tokens = tokenize(data[\"chatgpt_answer\"], lemmatized=True, remove_stopword=True, remove_punct = True)\n",
    "ref_tokens = tokenize(data[\"human_answer\"], lemmatized=True, remove_stopword=True, remove_punct = True)\n",
    "result = compute_sentiment(gen_tokens, \n",
    "                           ref_tokens, \n",
    "                           pos[0].values,\n",
    "                           neg[0].values)\n",
    "result.head()\n",
    "\n",
    "(result['gen_sentiment'] - result['ref_sentiment']).mean()\n",
    "\n",
    "res = wilcoxon(result['gen_sentiment'] - result['ref_sentiment'], alternative='greater')\n",
    "res.statistic, res.pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gen_sentiment</th>\n",
       "      <th>ref_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.777778</td>\n",
       "      <td>0.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.600000</td>\n",
       "      <td>-0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gen_sentiment  ref_sentiment\n",
       "0       0.000000      -0.500000\n",
       "1      -0.777778       0.076923\n",
       "2       0.666667       0.200000\n",
       "3       1.000000       0.200000\n",
       "4       0.600000      -0.333333"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.1462586239453829"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(10279.5, 0.0011456573663914912)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_tokens = tokenize(data[\"chatgpt_answer\"], lemmatized=False, remove_stopword=False, remove_punct = True)\n",
    "ref_tokens = tokenize(data[\"human_answer\"], lemmatized=False, remove_stopword=False, remove_punct = True)\n",
    "result = compute_sentiment(gen_tokens, \n",
    "                           ref_tokens, \n",
    "                           pos[0].values,\n",
    "                           neg[0].values)\n",
    "result.head()\n",
    "\n",
    "(result['gen_sentiment'] - result['ref_sentiment']).mean()\n",
    "\n",
    "res = wilcoxon(result['gen_sentiment'] - result['ref_sentiment'], alternative='greater')\n",
    "res.statistic, res.pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In general, which tokenization configuration should be used? Why does this combination make the most sense?\n",
    "The configuration that had the best results was tokenize(data, lemmatized=True, remove_stopword=True, remove_punct=True). This combination makes the most sense because lemmatization will help normalize the data by reducing the unique words in the dataset while still preserving semantic meaning of a given document in the corpus. Removing stop words and punctuation will also improve tokenization results since these types of characters don't add to the semantic meaning of the text. Stopwords and punctuation also tend to have a high frequency within a text, so by removing this noise it is much easier to extract the desired text characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do you think, overall, ChatGPT-generated answers are more posive or negative than human-generated ones? Use data to support your conclusion.\n",
    "\n",
    "As seen in the statistics from result.describe(), ChatGPT does have a lower mean value for sentiment, with the average human answer (0.233) around 3x more positive than ChatGPT (0.086). The 50th percentile of ChatGPT answers have a sentiment score of 0 or lower, while the human generated answers 50th percentile is 0.26. While they have similar average scores for the 25th percentile of responses, the human responses are clearly much more positive on average, a trend that continues for the 75th percentile with human responses scoring around 30% higher sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gen_sentiment</th>\n",
       "      <th>ref_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.232919</td>\n",
       "      <td>0.086660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.593347</td>\n",
       "      <td>0.563039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.255682</td>\n",
       "      <td>-0.253289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.261364</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.753676</td>\n",
       "      <td>0.446429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       gen_sentiment  ref_sentiment\n",
       "count     200.000000     200.000000\n",
       "mean        0.232919       0.086660\n",
       "std         0.593347       0.563039\n",
       "min        -1.000000      -1.000000\n",
       "25%        -0.255682      -0.253289\n",
       "50%         0.261364       0.000000\n",
       "75%         0.753676       0.446429\n",
       "max         1.000000       1.000000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3: Performance Evaluation\n",
    "\n",
    "\n",
    "Next, we evaluate how accurate the ChatGPT-generated answers are, compared to the human-generated answers. One widely used method is to calculate the `precision` and `recall` of n-grams. For simplicity, we only calculate bigrams here. You can try unigram, trigram, or n-grams in the same way.\n",
    "\n",
    "\n",
    "Define a funtion `bigram_precision_recall(gen_tokens, ref_tokens)` as follows:\n",
    "- take two parameters:\n",
    "    - `gen_tokens` is the tokenized ChatGPT-generated answers by the `tokenize` function in Q1.\n",
    "    - `ref_tokens` is the tokenized human answers by the `tokenize` function in Q1.\n",
    "- generate bigrams from each tokenized document in `gen_tokens` and `ref_tokens`\n",
    "- for each pair of ChatGPT-generated and human answers, find the overlapping bigrams between them\n",
    "- compute `precision` as the number of overlapping bigrams divided by the total number of bigrams from the ChatGPT-generated answer. In other words, the bigram is considered as a predicted value. The `precision` measures the percentage of correctly generated bigrams out of all generated bigrams.\n",
    "- compute `recall` as the number of overlapping bigrams divided by the total number of bigrams from the human answer. In other words, the `recall` measures the percentage of bigrams from the human answer can be successfully retrieved.\n",
    "- return the precision and recall for each pair of answers.\n",
    "\n",
    "\n",
    "Analysis: \n",
    "- Try different tokenization parameter configurations (lemmatized, remove_stopword, remove_punct), and observe how precison and recall change.\n",
    "- Do you think, in general, which tokenization configuration should be used? Why does this combination make the most senese?\n",
    "- Do you think, overall, ChatGPT is able to mimic human in answering these questions?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_precision_recall(gen_tokens, ref_tokens):\n",
    "    result = pd.DataFrame(columns = ['overlapping','precision','recall'])\n",
    "    \n",
    "    gen_bigrams = [list(nltk.bigrams(tokens)) for tokens in gen_tokens]\n",
    "    ref_bigrams = [list(nltk.bigrams(tokens)) for tokens in ref_tokens]\n",
    "\n",
    "    bigrams = list(zip(gen_bigrams, ref_bigrams))\n",
    "\n",
    "    overlapping = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    for gen, ref in bigrams:\n",
    "        overlap = [tup1 for tup1 in gen for tup2 in ref if tup1 == tup2]\n",
    "        overlapping.append(list(set(overlap)))\n",
    "        \n",
    "        if gen:\n",
    "            precision.append(len(overlap)/len(gen))\n",
    "        else:\n",
    "            precision.append(0)\n",
    "        \n",
    "        if ref:\n",
    "            recall.append(len(overlap)/len(ref))\n",
    "        else:\n",
    "            recall.append(0)\n",
    "\n",
    "    result['overlapping'] = overlapping\n",
    "    result['precision'] = precision\n",
    "    result['recall'] = recall\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overlapping</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[(it, goes), (to, pay)]</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.042553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[(can, be), (can, cancel), (out, .), (out, of)...</td>\n",
       "      <td>0.065421</td>\n",
       "      <td>0.031390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[(influence, the), (other, shareholders), (to,...</td>\n",
       "      <td>0.230216</td>\n",
       "      <td>0.096677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[(the, local), (be, the), (to, be)]</td>\n",
       "      <td>0.017647</td>\n",
       "      <td>0.051724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[(a, result), (as, a), (result, ,), (it, 's), ...</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.072165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         overlapping  precision    recall\n",
       "0                            [(it, goes), (to, pay)]   0.016807  0.042553\n",
       "1  [(can, be), (can, cancel), (out, .), (out, of)...   0.065421  0.031390\n",
       "2  [(influence, the), (other, shareholders), (to,...   0.230216  0.096677\n",
       "3                [(the, local), (be, the), (to, be)]   0.017647  0.051724\n",
       "4  [(a, result), (as, a), (result, ,), (it, 's), ...   0.028571  0.072165"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = bigram_precision_recall(gen_tokens, \n",
    "                                 ref_tokens)\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "precision    0.110050\n",
       "recall       0.165531\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[[\"precision\", \"recall\"]].mean(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "#### Try different tokenization parameter configurations (lemmatized, remove_stopword, remove_punct), and observe how precison and recall change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "precision    0.096335\n",
       "recall       0.153194\n",
       "dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_tokens = tokenize(data[\"chatgpt_answer\"], lemmatized=False, remove_stopword=False, remove_punct = True)\n",
    "ref_tokens = tokenize(data[\"human_answer\"], lemmatized=False, remove_stopword=False, remove_punct = True)\n",
    "result = bigram_precision_recall(gen_tokens, ref_tokens)\n",
    "result[[\"precision\", \"recall\"]].mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "precision    0.064452\n",
       "recall       0.101432\n",
       "dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_tokens = tokenize(data[\"chatgpt_answer\"], lemmatized=True, remove_stopword=True, remove_punct = True)\n",
    "ref_tokens = tokenize(data[\"human_answer\"], lemmatized=True, remove_stopword=True, remove_punct = True)\n",
    "result = bigram_precision_recall(gen_tokens, ref_tokens)\n",
    "result[[\"precision\", \"recall\"]].mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "precision    0.119037\n",
       "recall       0.186091\n",
       "dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_tokens = tokenize(data[\"chatgpt_answer\"], lemmatized=True, remove_stopword=False, remove_punct = True)\n",
    "ref_tokens = tokenize(data[\"human_answer\"], lemmatized=True, remove_stopword=False, remove_punct = True)\n",
    "result = bigram_precision_recall(gen_tokens, ref_tokens)\n",
    "result[[\"precision\", \"recall\"]].mean(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do you think, in general, which tokenization configuration should be used? Why does this combination make the most sense?\n",
    "The best configuration in this case was tokenize(data, lemmatized=True, remove_stopword=False, remove_punct=True). This combination makes sense because stopwords can provide context to certain words and their meaning or connections to other unique tokens, as well as identify phrases or make it easier to pair up tokens by pairing unique words and stopwords in a bigram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do you think, overall, ChatGPT is able to mimic human in answering these questions?\n",
    "I think ChatGPT is not mimicing humans very well, as it still has low precision and recall scores overall, regardless of the tokenization configuration. ChatGPT tends to repeat a lot of the words from the question, or rephrase the same information with some new text. The human answers have more variability in the text and more of a unique voice than the ones generated by ChatGPT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4 Compute TF-IDF\n",
    "\n",
    "Define a function `compute_tf_idf(tokenized_docs)` as follows: \n",
    "- Take paramter `tokenized_docs`, i.e., a list of tokenized documents by `tokenize` function in Q1\n",
    "- Calculate tf_idf weights as shown in lecture notes (Hint: feel free to reuse the code segment in Lecture Notes (II))\n",
    "- Return the smoothed normalized `tf_idf` array, where each row stands for a document and each column denotes a word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tfidf(docs):\n",
    "    # process all documents to get token frequency for each doc\n",
    "    docs_tokens = {idx:nltk.FreqDist(tokens) for idx,tokens in enumerate(docs)}\n",
    "\n",
    "    #get document-term matrix\n",
    "    dtm=pd.DataFrame.from_dict(docs_tokens, orient=\"index\").fillna(0).sort_index(axis = 0)\n",
    "      \n",
    "    # get term frequency matrix        \n",
    "    tf=dtm.values\n",
    "    doc_len=tf.sum(axis=1, keepdims=True)\n",
    "    tf=np.divide(tf, doc_len)\n",
    "    \n",
    "    # get idf\n",
    "    df=np.where(tf>0,1,0)\n",
    "    \n",
    "    # get smoothed and normalized tfidf\n",
    "    smoothed_idf=np.log(np.divide(len(docs)+1, np.sum(df, axis=0)+1))+1    \n",
    "    smoothed_tf_idf=normalize(tf*smoothed_idf)\n",
    "    \n",
    "    return smoothed_tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try different tokenization options to see how these options affect TFIDF matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.lemmatized=True, remove_stopword=False, remove_punct = True\n",
      " Shape: (200, 1438)\n",
      "\n",
      "2.lemmatized=True, remove_stopword=True, remove_punct = True:\n",
      " Shape: (200, 1271)\n",
      "\n",
      "3.lemmatized=False, remove_stopword=False, remove_punct = True:\n",
      " Shape: (200, 1643)\n",
      "\n",
      "4.lemmatized=False, remove_stopword=False, remove_punct = False:\n",
      " Shape: (200, 1665)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test tfidf generation using questions\n",
    "\n",
    "question_tokens = tokenize(data[\"question\"], lemmatized=True, remove_stopword=False, remove_punct = True)\n",
    "dtm = compute_tfidf(question_tokens)\n",
    "print(f\"1.lemmatized=True, remove_stopword=False, remove_punct = True\\n \\\n",
    "Shape: {dtm.shape}\\n\")\n",
    "\n",
    "question_tokens = tokenize(data[\"question\"], lemmatized=True, remove_stopword=True, remove_punct = True)\n",
    "dtm = compute_tfidf(question_tokens)\n",
    "print(f\"2.lemmatized=True, remove_stopword=True, remove_punct = True:\\n \\\n",
    "Shape: {dtm.shape}\\n\")\n",
    "\n",
    "question_tokens = tokenize(data[\"question\"], lemmatized=False, remove_stopword=False, remove_punct = True)\n",
    "dtm = compute_tfidf(question_tokens)\n",
    "print(f\"3.lemmatized=False, remove_stopword=False, remove_punct = True:\\n \\\n",
    "Shape: {dtm.shape}\\n\")\n",
    "\n",
    "question_tokens = tokenize(data[\"question\"], lemmatized=False, remove_stopword=False, remove_punct = False)\n",
    "dtm = compute_tfidf(question_tokens)\n",
    "print(f\"4.lemmatized=False, remove_stopword=False, remove_punct = False:\\n \\\n",
    "Shape: {dtm.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5. Assess similarity. \n",
    "\n",
    "\n",
    "Define a function `assess_similarity(question_tokens, gen_tokens, ref_tokens)`  as follows: \n",
    "- Take three inputs:\n",
    "   - `question_tokens`: tokenized questions by `tokenize` function in Q1\n",
    "   - `gen_tokens`: tokenized ChatGPT-generated answers by `tokenize` function in Q1\n",
    "   - `ref_tokens`: tokenized human answers by `tokenize` function in Q1\n",
    "- Concatenate these three token lists into a single list to form a corpus\n",
    "- Calculate the smoothed normalized tf_idf matrix for the concatenated list using the `compute_tfidf` function defined in Q3.\n",
    "- Split the tf_idf matrix into sub-matrices corresponding to `question_tokens`, `gen_tokens`, and `ref_tokens` respectively\n",
    "- For each question, find its similarities to the paired ChatGPT-generated answer and human answer.\n",
    "- For each pair of ChatGPT-generated answer and human answer, find their similarity\n",
    "- Print out the following:\n",
    "    - the question which has the largest similarity to the ChatGPT-generated answer.\n",
    "    - the question which has the largest similarity to the human answer.\n",
    "    - the pair of ChatGPT-generated and human answers which have the largest similarity.\n",
    "- Return a DataFrame with the three columns for the similarities among questions and answers.\n",
    "\n",
    "\n",
    "\n",
    "Analysis: \n",
    "- Try different tokenization parameter configurations (lemmatized, remove_stopword, remove_punct), and observe how similarities change.\n",
    "- Based on similarity, do you think ChatGPT-generate answers are more relevant to questions than human answers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def assess_similarity(question_tokens, gen_tokens, ref_tokens):\n",
    "    n_tokens = len(data)\n",
    "    # Concatenate the three lists into a single list\n",
    "    corpus = []\n",
    "    corpus.extend(question_tokens)\n",
    "    corpus.extend(gen_tokens)\n",
    "    corpus.extend(ref_tokens)\n",
    "    \n",
    "    tf_idf = compute_tfidf(corpus)\n",
    "    \n",
    "    # Split the tf-idf matrix\n",
    "    tfidf_question = tf_idf[:n_tokens]\n",
    "    tfidf_gen = tf_idf[n_tokens:n_tokens + n_tokens]\n",
    "    tfidf_ref = tf_idf[-n_tokens:]\n",
    "\n",
    "    # Calculate similarity\n",
    "    question_gen_sim = cosine_similarity(tfidf_question, tfidf_gen)\n",
    "    question_ref_sim = cosine_similarity(tfidf_question, tfidf_ref)\n",
    "    gen_ref_sim = cosine_similarity(tfidf_ref, tfidf_gen)\n",
    "\n",
    "    # Find the maximum similarities and their indices\n",
    "    max_sim_question_gen = np.max(question_gen_sim, axis=1)\n",
    "    question_gen_idx = np.argmax(max_sim_question_gen)\n",
    "    \n",
    "    max_sim_question_ref = np.max(question_ref_sim, axis=1)\n",
    "    question_ref_idx = np.argmax(max_sim_question_ref)\n",
    "    \n",
    "    max_sim_gen_ref = np.max(gen_ref_sim, axis = 1)\n",
    "    gen_ref_idx = np.argmax(max_sim_gen_ref)\n",
    "    \n",
    "    result = pd.DataFrame({\n",
    "        'question_ref_sim': max_sim_question_ref,\n",
    "        'question_gen_sim': max_sim_question_gen,\n",
    "        'gen_ref_sim': max_sim_gen_ref\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nQuestion with the largest similarity to the ChatGPT-generated answer:\\\n",
    "    \\nQuestion: {data['question'][question_gen_idx]}\\\n",
    "    \\nChatGPT: {data['chatgpt_answer'][question_gen_idx]}\\\n",
    "    \\nHuman: {data['human_answer'][question_gen_idx]}\")\n",
    "    print(f\"\\n{result.iloc[question_gen_idx]}\")\n",
    "    \n",
    "    print(f\"\\nQuestion with the largest similarity to the human answer:\\\n",
    "        \\nQuestion: {data['question'][question_ref_idx]}\\\n",
    "        \\nChatGPT: {data['chatgpt_answer'][question_ref_idx]}\\\n",
    "        \\nHuman: {data['human_answer'][question_ref_idx]}\")\n",
    "    print(f\"\\n{result.iloc[question_ref_idx]}\")\n",
    "    \n",
    "    print(f\"\\nQuestion with the largest similarity between ChatGPT-generated and human answers:\\\n",
    "        \\nQuestion: {data['question'][gen_ref_idx]}\\\n",
    "        \\nChatGPT: {data['chatgpt_answer'][gen_ref_idx]}\\\n",
    "        \\nHuman: {data['human_answer'][gen_ref_idx]}\")\n",
    "    print(f\"\\n{result.iloc[gen_ref_idx]}\")\n",
    "    \n",
    "    return result    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question with the largest similarity to the ChatGPT-generated answer:    \n",
      "Question: Where to find historical quotes for the Dow Jones Global Total Stock Market Index?    \n",
      "ChatGPT: You can find historical quotes for the Dow Jones Global Total Stock Market Index at several financial websites, such as Yahoo Finance, Google Finance, and Bloomberg. These websites allow you to view the historical performance of the index and see how it has changed over time.To find historical quotes for the Dow Jones Global Total Stock Market Index on Yahoo Finance, go to the Yahoo Finance website and enter \"Dow Jones Global Total Stock Market Index\" in the search bar. From the search results, click on the link for the Dow Jones Global Total Stock Market Index. On the resulting page, you will be able to view the current value of the index as well as historical data dating back to the index's inception.To find historical quotes for the Dow Jones Global Total Stock Market Index on Google Finance, go to the Google Finance website and enter \"Dow Jones Global Total Stock Market Index\" in the search bar. From the search results, click on the link for the Dow Jones Global Total Stock Market Index. On the resulting page, you will be able to view the current value of the index as well as a chart showing its historical performance.To find historical quotes for the Dow Jones Global Total Stock Market Index on Bloomberg, go to the Bloomberg website and enter \"Dow Jones Global Total Stock Market Index\" in the search bar. From the search results, click on the link for the Dow Jones Global Total Stock Market Index. On the resulting page, you will be able to view the current value of the index as well as a chart showing its historical performance.    \n",
      "Human: A number of places.  First, fast and cheap, you can probably get this from EODData.com, as part of a historical index price download -- they have good customer service in my experience and will likely confirm it for you before you buy.  Any number of other providers can get it for you too.  Likely Capital IQ, Bloomberg, and other professional solutions. I checked a number of free sites, and Market Watch was the only that had a longer history than a few months.\n",
      "\n",
      "question_ref_sim    0.190471\n",
      "question_gen_sim    0.782894\n",
      "gen_ref_sim         0.232009\n",
      "Name: 30, dtype: float64\n",
      "\n",
      "Question with the largest similarity to the human answer:        \n",
      "Question: when the wind blows james patterson        \n",
      "ChatGPT: \"When the Wind Blows\" is a novel by James Patterson, published in 1996. The book is a suspense thriller that follows a couple, Frannie and Kit, as they try to uncover the truth behind a series of strange and terrifying events that occur in their small town. The novel explores themes of government conspiracy, environmentalism, and the dangers of technology. It is the first book in the \"Maximum Ride\" series, which follows the adventures of a group of genetically enhanced young people known as the \"Flock,\" who have the ability to fly.        \n",
      "Human: When the Wind Blows is a novel by James Patterson .\n",
      "\n",
      "question_ref_sim    0.871841\n",
      "question_gen_sim    0.253087\n",
      "gen_ref_sim         0.353696\n",
      "Name: 117, dtype: float64\n",
      "\n",
      "Question with the largest similarity between ChatGPT-generated and human answers:        \n",
      "Question: what was the date of pearl harbor        \n",
      "ChatGPT: The attack on Pearl Harbor occurred on December 7, 1941. It was a surprise military strike by the Imperial Japanese Navy against the United States naval base at Pearl Harbor, Hawaii. The attack led to the United States entering World War II.        \n",
      "Human: The attack on Pearl Harbor (called Hawaii Operation or Operation AI by the Japanese Imperial General Headquarters (Operation Z in planning) and the Battle of Pearl Harbor) was a surprise military strike conducted by the Imperial Japanese Navy against the United States naval base at Pearl Harbor , Hawaii, on the morning of December 7, 1941 (December 8 in Japan).\n",
      "\n",
      "question_ref_sim    0.419917\n",
      "question_gen_sim    0.397606\n",
      "gen_ref_sim         0.682076\n",
      "Name: 122, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_ref_sim</th>\n",
       "      <th>question_gen_sim</th>\n",
       "      <th>gen_ref_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.139381</td>\n",
       "      <td>0.570549</td>\n",
       "      <td>0.190470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.204887</td>\n",
       "      <td>0.535899</td>\n",
       "      <td>0.287058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.215753</td>\n",
       "      <td>0.464515</td>\n",
       "      <td>0.451962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.158283</td>\n",
       "      <td>0.418185</td>\n",
       "      <td>0.350015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.178233</td>\n",
       "      <td>0.286313</td>\n",
       "      <td>0.202566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_ref_sim  question_gen_sim  gen_ref_sim\n",
       "0          0.139381          0.570549     0.190470\n",
       "1          0.204887          0.535899     0.287058\n",
       "2          0.215753          0.464515     0.451962\n",
       "3          0.158283          0.418185     0.350015\n",
       "4          0.178233          0.286313     0.202566"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = assess_similarity(question_tokens, gen_tokens, ref_tokens)\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_ref_sim</th>\n",
       "      <th>question_gen_sim</th>\n",
       "      <th>gen_ref_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.233769</td>\n",
       "      <td>0.352870</td>\n",
       "      <td>0.320368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.107624</td>\n",
       "      <td>0.110428</td>\n",
       "      <td>0.123144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.084628</td>\n",
       "      <td>0.090956</td>\n",
       "      <td>0.094002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.156987</td>\n",
       "      <td>0.276771</td>\n",
       "      <td>0.231383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.209210</td>\n",
       "      <td>0.353386</td>\n",
       "      <td>0.312536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.277172</td>\n",
       "      <td>0.431382</td>\n",
       "      <td>0.397675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.871841</td>\n",
       "      <td>0.782894</td>\n",
       "      <td>0.682076</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       question_ref_sim  question_gen_sim  gen_ref_sim\n",
       "count        200.000000        200.000000   200.000000\n",
       "mean           0.233769          0.352870     0.320368\n",
       "std            0.107624          0.110428     0.123144\n",
       "min            0.084628          0.090956     0.094002\n",
       "25%            0.156987          0.276771     0.231383\n",
       "50%            0.209210          0.353386     0.312536\n",
       "75%            0.277172          0.431382     0.397675\n",
       "max            0.871841          0.782894     0.682076"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "#### Try different tokenization parameter configurations (lemmatized, remove_stopword, remove_punct), and observe how similarities change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question with the largest similarity to the ChatGPT-generated answer:    \n",
      "Question: Where to find historical quotes for the Dow Jones Global Total Stock Market Index?    \n",
      "ChatGPT: You can find historical quotes for the Dow Jones Global Total Stock Market Index at several financial websites, such as Yahoo Finance, Google Finance, and Bloomberg. These websites allow you to view the historical performance of the index and see how it has changed over time.To find historical quotes for the Dow Jones Global Total Stock Market Index on Yahoo Finance, go to the Yahoo Finance website and enter \"Dow Jones Global Total Stock Market Index\" in the search bar. From the search results, click on the link for the Dow Jones Global Total Stock Market Index. On the resulting page, you will be able to view the current value of the index as well as historical data dating back to the index's inception.To find historical quotes for the Dow Jones Global Total Stock Market Index on Google Finance, go to the Google Finance website and enter \"Dow Jones Global Total Stock Market Index\" in the search bar. From the search results, click on the link for the Dow Jones Global Total Stock Market Index. On the resulting page, you will be able to view the current value of the index as well as a chart showing its historical performance.To find historical quotes for the Dow Jones Global Total Stock Market Index on Bloomberg, go to the Bloomberg website and enter \"Dow Jones Global Total Stock Market Index\" in the search bar. From the search results, click on the link for the Dow Jones Global Total Stock Market Index. On the resulting page, you will be able to view the current value of the index as well as a chart showing its historical performance.    \n",
      "Human: A number of places.  First, fast and cheap, you can probably get this from EODData.com, as part of a historical index price download -- they have good customer service in my experience and will likely confirm it for you before you buy.  Any number of other providers can get it for you too.  Likely Capital IQ, Bloomberg, and other professional solutions. I checked a number of free sites, and Market Watch was the only that had a longer history than a few months.\n",
      "\n",
      "question_ref_sim    0.195478\n",
      "question_gen_sim    0.789226\n",
      "gen_ref_sim         0.208271\n",
      "Name: 30, dtype: float64\n",
      "\n",
      "Question with the largest similarity to the human answer:        \n",
      "Question: when the wind blows james patterson        \n",
      "ChatGPT: \"When the Wind Blows\" is a novel by James Patterson, published in 1996. The book is a suspense thriller that follows a couple, Frannie and Kit, as they try to uncover the truth behind a series of strange and terrifying events that occur in their small town. The novel explores themes of government conspiracy, environmentalism, and the dangers of technology. It is the first book in the \"Maximum Ride\" series, which follows the adventures of a group of genetically enhanced young people known as the \"Flock,\" who have the ability to fly.        \n",
      "Human: When the Wind Blows is a novel by James Patterson .\n",
      "\n",
      "question_ref_sim    0.874726\n",
      "question_gen_sim    0.272008\n",
      "gen_ref_sim         0.373890\n",
      "Name: 117, dtype: float64\n",
      "\n",
      "Question with the largest similarity between ChatGPT-generated and human answers:        \n",
      "Question: what was the date of pearl harbor        \n",
      "ChatGPT: The attack on Pearl Harbor occurred on December 7, 1941. It was a surprise military strike by the Imperial Japanese Navy against the United States naval base at Pearl Harbor, Hawaii. The attack led to the United States entering World War II.        \n",
      "Human: The attack on Pearl Harbor (called Hawaii Operation or Operation AI by the Japanese Imperial General Headquarters (Operation Z in planning) and the Battle of Pearl Harbor) was a surprise military strike conducted by the Imperial Japanese Navy against the United States naval base at Pearl Harbor , Hawaii, on the morning of December 7, 1941 (December 8 in Japan).\n",
      "\n",
      "question_ref_sim    0.430420\n",
      "question_gen_sim    0.400385\n",
      "gen_ref_sim         0.696020\n",
      "Name: 122, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_ref_sim</th>\n",
       "      <th>question_gen_sim</th>\n",
       "      <th>gen_ref_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.215811</td>\n",
       "      <td>0.332533</td>\n",
       "      <td>0.305191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.108893</td>\n",
       "      <td>0.114270</td>\n",
       "      <td>0.128017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.077459</td>\n",
       "      <td>0.095774</td>\n",
       "      <td>0.087769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.133289</td>\n",
       "      <td>0.253999</td>\n",
       "      <td>0.206993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.193265</td>\n",
       "      <td>0.319195</td>\n",
       "      <td>0.296274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.264397</td>\n",
       "      <td>0.408473</td>\n",
       "      <td>0.382049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.874726</td>\n",
       "      <td>0.789226</td>\n",
       "      <td>0.696020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       question_ref_sim  question_gen_sim  gen_ref_sim\n",
       "count        200.000000        200.000000   200.000000\n",
       "mean           0.215811          0.332533     0.305191\n",
       "std            0.108893          0.114270     0.128017\n",
       "min            0.077459          0.095774     0.087769\n",
       "25%            0.133289          0.253999     0.206993\n",
       "50%            0.193265          0.319195     0.296274\n",
       "75%            0.264397          0.408473     0.382049\n",
       "max            0.874726          0.789226     0.696020"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_tokens = tokenize(data[\"chatgpt_answer\"], lemmatized=False, remove_stopword=False, remove_punct = True)\n",
    "ref_tokens = tokenize(data[\"human_answer\"], lemmatized=False, remove_stopword=False, remove_punct = True)\n",
    "result = assess_similarity(question_tokens, gen_tokens, ref_tokens)\n",
    "result.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question with the largest similarity to the ChatGPT-generated answer:    \n",
      "Question: Where to find historical quotes for the Dow Jones Global Total Stock Market Index?    \n",
      "ChatGPT: You can find historical quotes for the Dow Jones Global Total Stock Market Index at several financial websites, such as Yahoo Finance, Google Finance, and Bloomberg. These websites allow you to view the historical performance of the index and see how it has changed over time.To find historical quotes for the Dow Jones Global Total Stock Market Index on Yahoo Finance, go to the Yahoo Finance website and enter \"Dow Jones Global Total Stock Market Index\" in the search bar. From the search results, click on the link for the Dow Jones Global Total Stock Market Index. On the resulting page, you will be able to view the current value of the index as well as historical data dating back to the index's inception.To find historical quotes for the Dow Jones Global Total Stock Market Index on Google Finance, go to the Google Finance website and enter \"Dow Jones Global Total Stock Market Index\" in the search bar. From the search results, click on the link for the Dow Jones Global Total Stock Market Index. On the resulting page, you will be able to view the current value of the index as well as a chart showing its historical performance.To find historical quotes for the Dow Jones Global Total Stock Market Index on Bloomberg, go to the Bloomberg website and enter \"Dow Jones Global Total Stock Market Index\" in the search bar. From the search results, click on the link for the Dow Jones Global Total Stock Market Index. On the resulting page, you will be able to view the current value of the index as well as a chart showing its historical performance.    \n",
      "Human: A number of places.  First, fast and cheap, you can probably get this from EODData.com, as part of a historical index price download -- they have good customer service in my experience and will likely confirm it for you before you buy.  Any number of other providers can get it for you too.  Likely Capital IQ, Bloomberg, and other professional solutions. I checked a number of free sites, and Market Watch was the only that had a longer history than a few months.\n",
      "\n",
      "question_ref_sim    0.161308\n",
      "question_gen_sim    0.699193\n",
      "gen_ref_sim         0.161496\n",
      "Name: 30, dtype: float64\n",
      "\n",
      "Question with the largest similarity to the human answer:        \n",
      "Question: when the wind blows james patterson        \n",
      "ChatGPT: \"When the Wind Blows\" is a novel by James Patterson, published in 1996. The book is a suspense thriller that follows a couple, Frannie and Kit, as they try to uncover the truth behind a series of strange and terrifying events that occur in their small town. The novel explores themes of government conspiracy, environmentalism, and the dangers of technology. It is the first book in the \"Maximum Ride\" series, which follows the adventures of a group of genetically enhanced young people known as the \"Flock,\" who have the ability to fly.        \n",
      "Human: When the Wind Blows is a novel by James Patterson .\n",
      "\n",
      "question_ref_sim    0.823524\n",
      "question_gen_sim    0.257985\n",
      "gen_ref_sim         0.375390\n",
      "Name: 117, dtype: float64\n",
      "\n",
      "Question with the largest similarity between ChatGPT-generated and human answers:        \n",
      "Question: what people used mayan numeral system        \n",
      "ChatGPT: The Maya civilization, which flourished in Mesoamerica (present-day Mexico and Central America) from approximately 2000 BCE to 1500 CE, used a vigesimal (base-20) numeral system to represent numbers. The Maya numeral system was used for counting, calculating, and keeping track of time.\\n\\nThe Maya numeral system was based on a combination of bar and dot symbols. The basic unit of the Maya numeral system was the dot, which represented the number 1. A bar symbol represented the number 5. The Maya numeral system used a place value system similar to the decimal system used in the modern Western world. For example, the number 18 in the Maya numeral system was represented by three dots and a bar, while the number 300 was represented by a symbol that looked like a shell.\\n\\nThe Maya numeral system was used by the Maya civilization for a wide range of purposes, including calculating the dates of celestial events, recording historical events, and keeping track of time. The Maya numeral system was also used in trade and commerce, as well as in the measurement of land and other resources. Despite the fact that the Maya civilization collapsed over 500 years ago, the Maya numeral system continues to be studied by historians and researchers today for its insights into the culture and history of the Maya civilization.        \n",
      "Human: Maya numerals are a vigesimal ( base - twenty ) numeral system used by the Pre-Columbian Maya civilization .\n",
      "\n",
      "question_ref_sim    0.318769\n",
      "question_gen_sim    0.318160\n",
      "gen_ref_sim         0.725314\n",
      "Name: 37, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_ref_sim</th>\n",
       "      <th>question_gen_sim</th>\n",
       "      <th>gen_ref_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.156714</td>\n",
       "      <td>0.228628</td>\n",
       "      <td>0.307750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.107821</td>\n",
       "      <td>0.121010</td>\n",
       "      <td>0.159078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.027274</td>\n",
       "      <td>0.030997</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.084312</td>\n",
       "      <td>0.141589</td>\n",
       "      <td>0.178442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.123175</td>\n",
       "      <td>0.210139</td>\n",
       "      <td>0.289394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.206487</td>\n",
       "      <td>0.311293</td>\n",
       "      <td>0.415362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.823524</td>\n",
       "      <td>0.699193</td>\n",
       "      <td>0.725314</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       question_ref_sim  question_gen_sim  gen_ref_sim\n",
       "count        200.000000        200.000000   200.000000\n",
       "mean           0.156714          0.228628     0.307750\n",
       "std            0.107821          0.121010     0.159078\n",
       "min            0.027274          0.030997     0.000000\n",
       "25%            0.084312          0.141589     0.178442\n",
       "50%            0.123175          0.210139     0.289394\n",
       "75%            0.206487          0.311293     0.415362\n",
       "max            0.823524          0.699193     0.725314"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_tokens = tokenize(data[\"chatgpt_answer\"], lemmatized=True, remove_stopword=True, remove_punct = True)\n",
    "ref_tokens = tokenize(data[\"human_answer\"], lemmatized=True, remove_stopword=True, remove_punct = True)\n",
    "result = assess_similarity(question_tokens, gen_tokens, ref_tokens)\n",
    "result.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question with the largest similarity to the ChatGPT-generated answer:    \n",
      "Question: Where to find historical quotes for the Dow Jones Global Total Stock Market Index?    \n",
      "ChatGPT: You can find historical quotes for the Dow Jones Global Total Stock Market Index at several financial websites, such as Yahoo Finance, Google Finance, and Bloomberg. These websites allow you to view the historical performance of the index and see how it has changed over time.To find historical quotes for the Dow Jones Global Total Stock Market Index on Yahoo Finance, go to the Yahoo Finance website and enter \"Dow Jones Global Total Stock Market Index\" in the search bar. From the search results, click on the link for the Dow Jones Global Total Stock Market Index. On the resulting page, you will be able to view the current value of the index as well as historical data dating back to the index's inception.To find historical quotes for the Dow Jones Global Total Stock Market Index on Google Finance, go to the Google Finance website and enter \"Dow Jones Global Total Stock Market Index\" in the search bar. From the search results, click on the link for the Dow Jones Global Total Stock Market Index. On the resulting page, you will be able to view the current value of the index as well as a chart showing its historical performance.To find historical quotes for the Dow Jones Global Total Stock Market Index on Bloomberg, go to the Bloomberg website and enter \"Dow Jones Global Total Stock Market Index\" in the search bar. From the search results, click on the link for the Dow Jones Global Total Stock Market Index. On the resulting page, you will be able to view the current value of the index as well as a chart showing its historical performance.    \n",
      "Human: A number of places.  First, fast and cheap, you can probably get this from EODData.com, as part of a historical index price download -- they have good customer service in my experience and will likely confirm it for you before you buy.  Any number of other providers can get it for you too.  Likely Capital IQ, Bloomberg, and other professional solutions. I checked a number of free sites, and Market Watch was the only that had a longer history than a few months.\n",
      "\n",
      "question_ref_sim    0.189161\n",
      "question_gen_sim    0.736404\n",
      "gen_ref_sim         0.216249\n",
      "Name: 30, dtype: float64\n",
      "\n",
      "Question with the largest similarity to the human answer:        \n",
      "Question: when the wind blows james patterson        \n",
      "ChatGPT: \"When the Wind Blows\" is a novel by James Patterson, published in 1996. The book is a suspense thriller that follows a couple, Frannie and Kit, as they try to uncover the truth behind a series of strange and terrifying events that occur in their small town. The novel explores themes of government conspiracy, environmentalism, and the dangers of technology. It is the first book in the \"Maximum Ride\" series, which follows the adventures of a group of genetically enhanced young people known as the \"Flock,\" who have the ability to fly.        \n",
      "Human: When the Wind Blows is a novel by James Patterson .\n",
      "\n",
      "question_ref_sim    0.876957\n",
      "question_gen_sim    0.283533\n",
      "gen_ref_sim         0.387669\n",
      "Name: 117, dtype: float64\n",
      "\n",
      "Question with the largest similarity between ChatGPT-generated and human answers:        \n",
      "Question: what was the date of pearl harbor        \n",
      "ChatGPT: The attack on Pearl Harbor occurred on December 7, 1941. It was a surprise military strike by the Imperial Japanese Navy against the United States naval base at Pearl Harbor, Hawaii. The attack led to the United States entering World War II.        \n",
      "Human: The attack on Pearl Harbor (called Hawaii Operation or Operation AI by the Japanese Imperial General Headquarters (Operation Z in planning) and the Battle of Pearl Harbor) was a surprise military strike conducted by the Imperial Japanese Navy against the United States naval base at Pearl Harbor , Hawaii, on the morning of December 7, 1941 (December 8 in Japan).\n",
      "\n",
      "question_ref_sim    0.410304\n",
      "question_gen_sim    0.380548\n",
      "gen_ref_sim         0.718917\n",
      "Name: 122, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_ref_sim</th>\n",
       "      <th>question_gen_sim</th>\n",
       "      <th>gen_ref_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.198715</td>\n",
       "      <td>0.272580</td>\n",
       "      <td>0.354096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.101307</td>\n",
       "      <td>0.118067</td>\n",
       "      <td>0.136537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.064813</td>\n",
       "      <td>0.065961</td>\n",
       "      <td>0.108840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.124651</td>\n",
       "      <td>0.182670</td>\n",
       "      <td>0.239965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.175704</td>\n",
       "      <td>0.256312</td>\n",
       "      <td>0.345645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.253095</td>\n",
       "      <td>0.343357</td>\n",
       "      <td>0.452354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.876957</td>\n",
       "      <td>0.736404</td>\n",
       "      <td>0.718917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       question_ref_sim  question_gen_sim  gen_ref_sim\n",
       "count        200.000000        200.000000   200.000000\n",
       "mean           0.198715          0.272580     0.354096\n",
       "std            0.101307          0.118067     0.136537\n",
       "min            0.064813          0.065961     0.108840\n",
       "25%            0.124651          0.182670     0.239965\n",
       "50%            0.175704          0.256312     0.345645\n",
       "75%            0.253095          0.343357     0.452354\n",
       "max            0.876957          0.736404     0.718917"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_tokens = tokenize(data[\"chatgpt_answer\"], lemmatized=True, remove_stopword=False, remove_punct = True)\n",
    "ref_tokens = tokenize(data[\"human_answer\"], lemmatized=True, remove_stopword=False, remove_punct = True)\n",
    "result = assess_similarity(question_tokens, gen_tokens, ref_tokens)\n",
    "result.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Based on similarity, do you think ChatGPT-generate answers are more relevant to questions than human answers?\n",
    "The ChatGPT answers were on average closer in similarity to the question than the human answers. However, the human answers did have a higher max similarity for all configurations, suggesting the human answers may sometimes not offer enough detail in comparison to ChatGPT. This can be seen in the question with the highest similarity to the human answer, where the human answer is one line and ChatGPT gives a long paragraph. The question provided is also pretty vague, and is more of a statement than an actual question. ChatGPT does better in terms of providing information when the prompt is vague, but which response is more relevant to the user is unknown. ChatGPT does produce more information in comparision to humans that is relevant to the question, which not unexpected considering the huge training dataset available to ChatGPT while formulating its response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5 (Bonus): Further Analysis (Open question)\n",
    "\n",
    "\n",
    "- Can you find at least three significant differences between ChatGPT-generated and human answeres? Use data to support your answer.\n",
    "- Based on these differences, are you able to design a classifier to identify ChatGPT generated answers? Implement your ideas using traditional machine learning models, such as SVM, decision trees.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
