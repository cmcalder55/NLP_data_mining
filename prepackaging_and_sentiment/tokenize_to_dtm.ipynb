{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Define a function to analyze word counts in an input sentence \n",
    "\n",
    "\n",
    "Define a function named `tokenize(text)` which does the following:\n",
    "* accepts a sentence (i.e., `text` parameter) as an input\n",
    "* splits the sentence into a list of tokens by **space** (including tab, and new line). \n",
    "    - e.g., `it's a hello world!!!` will be split into tokens `[\"it's\", \"a\",\"hello\",\"world!!!\"]`  \n",
    "* removes the **leading/trailing punctuations or spaces** of each token, if any\n",
    "    - e.g., `world!!! -> world`, while `it's` does not change\n",
    "    - hint, you can import module *string*, use `string.punctuation` to get a list of punctuations (say `puncts`), and then use function `strip(puncts)` to remove leading or trailing punctuations in each token\n",
    "* only keeps tokens with 2 or more characters, i.e. `len(token)>1` \n",
    "* converts all tokens into lower case \n",
    "* find the count of each unique token and save the counts as dictionary, i.e., `{world: 1, a: 1, ...}`\n",
    "* returns the dictionary \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \n",
    "    punct = punctuation + '\\u201c\\u201d\\u2018\\u2019'\n",
    "    \n",
    "    # use split() with defaults to split by consecutive white spaces\n",
    "    all_tokens = text.split()\n",
    "    \n",
    "    # Remove any leading or trailing punctuation and spaces/tabs/new lines from each \n",
    "    tokens_clean = [t.strip(punct) for t in all_tokens]\n",
    "    \n",
    "    # If a word is longer than 2 letters, covert to lower case and keep\n",
    "    tokens = [t.lower() for t in tokens_clean if len(t) > 1]\n",
    "    \n",
    "    # Store word count in vocab dict\n",
    "    vocab = {}\n",
    "    for t in tokens:\n",
    "        if t not in vocab:\n",
    "            vocab[t] = 1\n",
    "        else:\n",
    "            vocab[t] += 1\n",
    "\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"it's\": 1, 'hello': 2, 'world': 2, 'it': 1, 'is': 1, 'again': 1}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test your code\n",
    "\n",
    "text = \"\"\"it's a Hello World!!!\n",
    "           it is Hello World again.\"\"\"\n",
    "\n",
    "tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Generate the vocabulary for a list of sentences.\n",
    "\n",
    "- accepts **a list of sentences**, i.e., `sents`, as an input, this will be your corpus\n",
    "- uses `tokenize` function you defined in Q1 to get the count dictionary for each sentence\n",
    "- build a large vocabulary for all sentences (entire corpus), where the keys are the unique words, and the values are the counts for these words in all the sentences.\n",
    "- sort the large dictionary by word count in descending order\n",
    "- return the large vocabulary for this corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vocab(sents):\n",
    "    \n",
    "    all_vocab = {}\n",
    "\n",
    "    # Iterate through series to get count dict for each sentence\n",
    "    for s in sents:\n",
    "        vocab_ct = tokenize(s)\n",
    "        \n",
    "        # Update dict of all words based on the words and their count per sentence\n",
    "        for v in vocab_ct:\n",
    "            if v not in all_vocab:\n",
    "                all_vocab[v] = vocab_ct[v]\n",
    "            else:\n",
    "                all_vocab[v] = all_vocab[v] + vocab_ct[v]\n",
    "\n",
    "    # convert key/val pairs to item tuples, reverse sort by word count, then convert back to dictionary\n",
    "    all_vocab = dict(sorted(all_vocab.items(), key = lambda v:v[1], reverse=True))\n",
    "    \n",
    "    return all_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Power of Natural Language Processing.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Until recently, the conventional wisdom was th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>But in the past two years language-based AI ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It has been used to write an article for The G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AI even excels at cognitive tasks like program...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Language-Based AI Tools Are Here to Stay.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Powerful generalizable language-based AI tools...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>To begin preparing now, start understanding yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Aggressively adopt new language-based AI techn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>And don’t forget to adopt these technologies y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text\n",
       "0           The Power of Natural Language Processing.\n",
       "1   Until recently, the conventional wisdom was th...\n",
       "2   But in the past two years language-based AI ha...\n",
       "3   It has been used to write an article for The G...\n",
       "4   AI even excels at cognitive tasks like program...\n",
       "..                                                ...\n",
       "73          Language-Based AI Tools Are Here to Stay.\n",
       "74  Powerful generalizable language-based AI tools...\n",
       "75  To begin preparing now, start understanding yo...\n",
       "76  Aggressively adopt new language-based AI techn...\n",
       "77  And don’t forget to adopt these technologies y...\n",
       "\n",
       "[78 rows x 1 columns]"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A test document. This document can be found at https://hbr.org/2022/04/the-power-of-natural-language-processing\n",
    "sents = pd.read_csv(\"sents.csv\")\n",
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 68,\n",
       " 'to': 65,\n",
       " 'and': 52,\n",
       " 'of': 50,\n",
       " 'for': 37,\n",
       " 'ai': 25,\n",
       " 'in': 24,\n",
       " 'is': 23,\n",
       " 'are': 22,\n",
       " 'tasks': 20,\n",
       " 'like': 20,\n",
       " 'be': 20,\n",
       " 'your': 20,\n",
       " 'data': 19,\n",
       " 'but': 18,\n",
       " 'that': 17,\n",
       " 'can': 17,\n",
       " 'it': 15,\n",
       " 'this': 14,\n",
       " 'you': 14,\n",
       " 'language': 13,\n",
       " 'on': 13,\n",
       " 'tools': 13,\n",
       " 'models': 12,\n",
       " 'or': 11,\n",
       " 'an': 10,\n",
       " 'from': 10,\n",
       " 'text': 10,\n",
       " 'research': 10,\n",
       " 'not': 10,\n",
       " 'how': 9,\n",
       " 'will': 9,\n",
       " 'language-based': 8,\n",
       " 'has': 8,\n",
       " 'even': 8,\n",
       " 'may': 8,\n",
       " 'more': 8,\n",
       " 'model': 8,\n",
       " 'was': 7,\n",
       " 'better': 7,\n",
       " 'at': 7,\n",
       " 'still': 7,\n",
       " 'do': 7,\n",
       " 'gpt-3': 7,\n",
       " 'which': 7,\n",
       " 'as': 7,\n",
       " 'work': 7,\n",
       " 'my': 7,\n",
       " 'elicit': 7,\n",
       " 'by': 6,\n",
       " 'have': 6,\n",
       " 'these': 6,\n",
       " 'they': 6,\n",
       " 'with': 6,\n",
       " 'transformative': 6,\n",
       " 'their': 6,\n",
       " 'foundation': 6,\n",
       " 'other': 6,\n",
       " 'potential': 6,\n",
       " 'now': 6,\n",
       " 'organizations': 6,\n",
       " 'assets': 6,\n",
       " 'value': 6,\n",
       " 'organization': 6,\n",
       " 'processing': 5,\n",
       " 'than': 5,\n",
       " 'humans': 5,\n",
       " 'advanced': 5,\n",
       " 'most': 5,\n",
       " 'been': 5,\n",
       " 'nlp': 5,\n",
       " 'used': 5,\n",
       " 'such': 5,\n",
       " 'already': 5,\n",
       " 'because': 5,\n",
       " 'latest': 5,\n",
       " 'programmers': 5,\n",
       " 'some': 5,\n",
       " 'new': 5,\n",
       " 'understanding': 5,\n",
       " 'many': 5,\n",
       " 'business': 5,\n",
       " 'intelligence': 5,\n",
       " 'natural': 4,\n",
       " 'while': 4,\n",
       " 'cognitive': 4,\n",
       " 'what': 4,\n",
       " 'programming': 4,\n",
       " 'tech': 4,\n",
       " 'tool': 4,\n",
       " 'openai': 4,\n",
       " 'next': 4,\n",
       " 'analytics': 4,\n",
       " 'transform': 4,\n",
       " 'could': 4,\n",
       " 'brainstorming': 4,\n",
       " 'results': 4,\n",
       " 'its': 4,\n",
       " 'need': 4,\n",
       " 'start': 4,\n",
       " 'don’t': 4,\n",
       " 'technologies': 4,\n",
       " 'just': 4,\n",
       " 'general': 4,\n",
       " 'artificial': 4,\n",
       " 'recently': 3,\n",
       " 'years': 3,\n",
       " 'called': 3,\n",
       " 'generate': 3,\n",
       " 'video': 3,\n",
       " 'best': 3,\n",
       " 'well': 3,\n",
       " 'questions': 3,\n",
       " 'traditional': 3,\n",
       " 'much': 3,\n",
       " 'google’s': 3,\n",
       " 'reasoning': 3,\n",
       " 'assistant': 3,\n",
       " 'simply': 3,\n",
       " 'nature': 3,\n",
       " 'critical': 3,\n",
       " 'also': 3,\n",
       " 'lead': 3,\n",
       " 'researchers': 3,\n",
       " 'number': 3,\n",
       " 'future': 3,\n",
       " 'useful': 3,\n",
       " 'variety': 3,\n",
       " 'all': 3,\n",
       " 'into': 3,\n",
       " 'step': 3,\n",
       " 'specialized': 3,\n",
       " 'might': 3,\n",
       " 'replace': 3,\n",
       " 'roles': 3,\n",
       " 'begin': 3,\n",
       " 'understand': 3,\n",
       " 'tremendous': 3,\n",
       " 'adopt': 3,\n",
       " 'poised': 3,\n",
       " 'managers': 3,\n",
       " 'society': 3,\n",
       " 'risks': 3,\n",
       " 'supply': 3,\n",
       " 'chain': 3,\n",
       " 'crisis': 3,\n",
       " 'decision': 2,\n",
       " 'changing': 2,\n",
       " 'process': 2,\n",
       " 'article': 2,\n",
       " 'possible': 2,\n",
       " 'where': 2,\n",
       " 'simple': 2,\n",
       " 'human': 2,\n",
       " 'instructions': 2,\n",
       " 'yet': 2,\n",
       " 'attention': 2,\n",
       " 'businesses': 2,\n",
       " 'predict': 2,\n",
       " 'word': 2,\n",
       " 'reports': 2,\n",
       " 'any': 2,\n",
       " 'previous': 2,\n",
       " 'first': 2,\n",
       " 'large': 2,\n",
       " 'shows': 2,\n",
       " 'three': 2,\n",
       " 'writing': 2,\n",
       " 'gpt-3-based': 2,\n",
       " 'intended': 2,\n",
       " 'generating': 2,\n",
       " 'codex': 2,\n",
       " 'creating': 2,\n",
       " 'jobs': 2,\n",
       " 'continue': 2,\n",
       " 'deepmind': 2,\n",
       " 'example': 2,\n",
       " 'thinking': 2,\n",
       " 'skills': 2,\n",
       " 'emerging': 2,\n",
       " 'images': 2,\n",
       " 'trained': 2,\n",
       " 'time': 2,\n",
       " 'openai’s': 2,\n",
       " 'due': 2,\n",
       " 'part': 2,\n",
       " 'economic': 2,\n",
       " 'growth': 2,\n",
       " 'similar': 2,\n",
       " 'i’ve': 2,\n",
       " 'assist': 2,\n",
       " 'am': 2,\n",
       " 'currently': 2,\n",
       " 'ought': 2,\n",
       " 'specific': 2,\n",
       " 'relevant': 2,\n",
       " 'summarization': 2,\n",
       " 'labeling': 2,\n",
       " 'literature': 2,\n",
       " 'others': 2,\n",
       " 'noisy': 2,\n",
       " 'overlooked': 2,\n",
       " 'topics': 2,\n",
       " 'adoption': 2,\n",
       " 'valuable': 2,\n",
       " 'find': 2,\n",
       " 'when': 2,\n",
       " 'existing': 2,\n",
       " 'google': 2,\n",
       " 'beginning': 2,\n",
       " 'experience': 2,\n",
       " 'inspired': 2,\n",
       " 'strategic': 2,\n",
       " 'identify': 2,\n",
       " 'add': 2,\n",
       " 'firm': 2,\n",
       " 'certainly': 2,\n",
       " 'if': 2,\n",
       " 'throughout': 2,\n",
       " 'customer': 2,\n",
       " 'think': 2,\n",
       " 'there': 2,\n",
       " 'training': 2,\n",
       " 'needs': 2,\n",
       " 'fewer': 2,\n",
       " 'applications': 2,\n",
       " 'firms': 2,\n",
       " 'only': 2,\n",
       " 'sectors': 2,\n",
       " 'divisions': 2,\n",
       " 'use': 2,\n",
       " 'finance': 2,\n",
       " 'want': 2,\n",
       " 'make': 2,\n",
       " 'decisions': 2,\n",
       " 'reorganize': 2,\n",
       " 'skilled': 2,\n",
       " 'labor': 2,\n",
       " 'likely': 2,\n",
       " 'employees': 2,\n",
       " 'increasing': 2,\n",
       " 'significant': 2,\n",
       " 'right': 2,\n",
       " 'ways': 2,\n",
       " 'suggestion': 2,\n",
       " 'had': 2,\n",
       " 'current': 2,\n",
       " 'difficult': 2,\n",
       " 'different': 2,\n",
       " 'way': 2,\n",
       " 'leaders': 2,\n",
       " 'who': 2,\n",
       " 'progress': 2,\n",
       " 'disruptive': 2,\n",
       " 'radically': 2,\n",
       " 'pandemic': 2,\n",
       " 'effects': 2,\n",
       " 'preparing': 2,\n",
       " 'here': 2,\n",
       " 'power': 1,\n",
       " 'until': 1,\n",
       " 'conventional': 1,\n",
       " 'wisdom': 1,\n",
       " 'data-driven': 1,\n",
       " 'making': 1,\n",
       " 'inferior': 1,\n",
       " 'creative': 1,\n",
       " 'ones': 1,\n",
       " 'past': 1,\n",
       " 'two': 1,\n",
       " 'leaps': 1,\n",
       " 'bounds': 1,\n",
       " 'common': 1,\n",
       " 'notions': 1,\n",
       " 'technology': 1,\n",
       " 'do.the': 1,\n",
       " 'visible': 1,\n",
       " 'advances': 1,\n",
       " 'what’s': 1,\n",
       " 'branch': 1,\n",
       " 'focused': 1,\n",
       " 'computers': 1,\n",
       " 'write': 1,\n",
       " 'guardian': 1,\n",
       " 'ai-authored': 1,\n",
       " 'blog': 1,\n",
       " 'posts': 1,\n",
       " 'gone': 1,\n",
       " 'viral': 1,\n",
       " 'feats': 1,\n",
       " 'weren’t': 1,\n",
       " 'few': 1,\n",
       " 'ago': 1,\n",
       " 'excels': 1,\n",
       " 'able': 1,\n",
       " 'programs': 1,\n",
       " 'games': 1,\n",
       " 'stunts': 1,\n",
       " 'grabbing': 1,\n",
       " 'really': 1,\n",
       " 'indicative': 1,\n",
       " 'known': 1,\n",
       " 'uses': 1,\n",
       " 'statistics': 1,\n",
       " 'sentence': 1,\n",
       " 'based': 1,\n",
       " 'preceding': 1,\n",
       " 'words': 1,\n",
       " 'practitioners': 1,\n",
       " 'call': 1,\n",
       " 'classifying': 1,\n",
       " 'documents': 1,\n",
       " 'analyzing': 1,\n",
       " 'sentiment': 1,\n",
       " 'blocks': 1,\n",
       " 'answering': 1,\n",
       " 'summarizing': 1,\n",
       " 'reshaping': 1,\n",
       " 'especially': 1,\n",
       " 'pivotal': 1,\n",
       " '10x': 1,\n",
       " 'larger': 1,\n",
       " 'upon': 1,\n",
       " 'release': 1,\n",
       " 'enabled': 1,\n",
       " 'perform': 1,\n",
       " 'solving': 1,\n",
       " 'high': 1,\n",
       " 'school–level': 1,\n",
       " 'math': 1,\n",
       " 'problems': 1,\n",
       " 'version': 1,\n",
       " 'instructgpt': 1,\n",
       " 'fine-tuned': 1,\n",
       " 'responses': 1,\n",
       " 'aligned': 1,\n",
       " 'values': 1,\n",
       " 'user': 1,\n",
       " 'intentions': 1,\n",
       " 'further': 1,\n",
       " 'impressive': 1,\n",
       " 'breakthroughs': 1,\n",
       " 'areas': 1,\n",
       " 'appeared': 1,\n",
       " 'promising': 1,\n",
       " 'coding': 1,\n",
       " 'discipline-specific': 1,\n",
       " 'microsoft-funded': 1,\n",
       " 'creator': 1,\n",
       " 'developed': 1,\n",
       " 'act': 1,\n",
       " 'code': 1,\n",
       " 'input': 1,\n",
       " 'powering': 1,\n",
       " 'products': 1,\n",
       " 'copilot': 1,\n",
       " 'microsoft’s': 1,\n",
       " 'subsidiary': 1,\n",
       " 'github': 1,\n",
       " 'capable': 1,\n",
       " 'basic': 1,\n",
       " 'game': 1,\n",
       " 'typing': 1,\n",
       " 'capability': 1,\n",
       " 'expected': 1,\n",
       " 'change': 1,\n",
       " 'improve': 1,\n",
       " 'lab': 1,\n",
       " 'demonstrates': 1,\n",
       " 'logic': 1,\n",
       " 'necessary': 1,\n",
       " 'outperform': 1,\n",
       " 'competitions': 1,\n",
       " 'considered': 1,\n",
       " 'area': 1,\n",
       " 'types': 1,\n",
       " 'multiple': 1,\n",
       " 'forms': 1,\n",
       " 'same': 1,\n",
       " 'dall·e': 1,\n",
       " 'high-resolution': 1,\n",
       " 'renderings': 1,\n",
       " 'imaginary': 1,\n",
       " 'scenes': 1,\n",
       " 'objects': 1,\n",
       " 'prompts': 1,\n",
       " 'economists': 1,\n",
       " 'expect': 1,\n",
       " 'affect': 1,\n",
       " 'every': 1,\n",
       " 'economy': 1,\n",
       " 'increases': 1,\n",
       " 'industrial': 1,\n",
       " 'revolution': 1,\n",
       " 'own': 1,\n",
       " 'looking': 1,\n",
       " 'working': 1,\n",
       " 'san': 1,\n",
       " 'francisco': 1,\n",
       " 'company': 1,\n",
       " 'developing': 1,\n",
       " 'open-ended': 1,\n",
       " 'help': 1,\n",
       " 'answer': 1,\n",
       " 'minutes': 1,\n",
       " 'hours': 1,\n",
       " 'instead': 1,\n",
       " 'weeks': 1,\n",
       " 'months': 1,\n",
       " 'designed': 1,\n",
       " 'growing': 1,\n",
       " 'rephrasing': 1,\n",
       " 'reviews': 1,\n",
       " 'found': 1,\n",
       " 'surprisingly': 1,\n",
       " 'works': 1,\n",
       " 'rough': 1,\n",
       " 'around': 1,\n",
       " 'edges': 1,\n",
       " 'spotty': 1,\n",
       " 'accuracy': 1,\n",
       " 'promise': 1,\n",
       " 'rephrase': 1,\n",
       " 'task': 1,\n",
       " 'lack': 1,\n",
       " 'integration': 1,\n",
       " 'apps': 1,\n",
       " 'renders': 1,\n",
       " 'impractical': 1,\n",
       " 'great': 1,\n",
       " 'ideas': 1,\n",
       " 'identifying': 1,\n",
       " 'despite': 1,\n",
       " 'barriers': 1,\n",
       " 'situations': 1,\n",
       " 'offers': 1,\n",
       " 'review': 1,\n",
       " 'sort': 1,\n",
       " 'bread-and-butter': 1,\n",
       " 'digging': 1,\n",
       " 'topic': 1,\n",
       " 'become': 1,\n",
       " 'go-to': 1,\n",
       " 'resource': 1,\n",
       " 'spend': 1,\n",
       " 'less': 1,\n",
       " 'trying': 1,\n",
       " 'content': 1,\n",
       " 'applicable': 1,\n",
       " 'interfaces': 1,\n",
       " 'academic': 1,\n",
       " 'search': 1,\n",
       " 'scholar': 1,\n",
       " 'integrate': 1,\n",
       " 'seeks': 1,\n",
       " 'utilize': 1,\n",
       " 'supporting': 1,\n",
       " 'planning': 1,\n",
       " 'prepare': 1,\n",
       " 'determine': 1,\n",
       " 'techniques': 1,\n",
       " 'leveraged': 1,\n",
       " 'aware': 1,\n",
       " 'overlooking': 1,\n",
       " 'essential': 1,\n",
       " 'utilizing': 1,\n",
       " 'management': 1,\n",
       " 'voice': 1,\n",
       " 'about': 1,\n",
       " 'emails': 1,\n",
       " 'analysts': 1,\n",
       " 'contracts': 1,\n",
       " 'press': 1,\n",
       " 'releases': 1,\n",
       " 'archives': 1,\n",
       " 'meetings': 1,\n",
       " 'phone': 1,\n",
       " 'calls': 1,\n",
       " 'transcribed': 1,\n",
       " 'so': 1,\n",
       " 'extract': 1,\n",
       " 'hugging': 1,\n",
       " 'face': 1,\n",
       " 'startup': 1,\n",
       " 'released': 1,\n",
       " 'autonlp': 1,\n",
       " 'automates': 1,\n",
       " 'standard': 1,\n",
       " 'uploading': 1,\n",
       " 'platform': 1,\n",
       " 'labels': 1,\n",
       " 'far': 1,\n",
       " 'made': 1,\n",
       " 'ambitious': 1,\n",
       " 'bets': 1,\n",
       " 'struggle': 1,\n",
       " 'drive': 1,\n",
       " 'core': 1,\n",
       " 'remain': 1,\n",
       " 'cautious': 1,\n",
       " 'overzealous': 1,\n",
       " 'good': 1,\n",
       " 'machine': 1,\n",
       " 'learning': 1,\n",
       " 'engineers': 1,\n",
       " 'talented': 1,\n",
       " 'scientists': 1,\n",
       " 'manage': 1,\n",
       " 'take': 1,\n",
       " 'again': 1,\n",
       " 'within': 1,\n",
       " 'highly': 1,\n",
       " 'vocabularies': 1,\n",
       " 'through': 1,\n",
       " 'combination': 1,\n",
       " 'open': 1,\n",
       " 'datasets': 1,\n",
       " 'train': 1,\n",
       " 'customized': 1,\n",
       " 'commercial': 1,\n",
       " 'banking': 1,\n",
       " 'capital': 1,\n",
       " 'markets': 1,\n",
       " 'unlabeled': 1,\n",
       " 'unlock': 1,\n",
       " 'untold': 1,\n",
       " 'firm.understand': 1,\n",
       " 'leverage': 1,\n",
       " 'ai-based': 1,\n",
       " 'won’t': 1,\n",
       " 'automate': 1,\n",
       " 'makers': 1,\n",
       " 'startups': 1,\n",
       " 'verneek': 1,\n",
       " 'elicit-like': 1,\n",
       " 'enable': 1,\n",
       " 'everyone': 1,\n",
       " 'data-informed': 1,\n",
       " 'transcend': 1,\n",
       " 'generation': 1,\n",
       " 'productive': 1,\n",
       " 'means': 1,\n",
       " 'dedicated': 1,\n",
       " 'modest': 1,\n",
       " 'using': 1,\n",
       " 'them': 1,\n",
       " 'complex': 1,\n",
       " 'true': 1,\n",
       " 'software': 1,\n",
       " 'developers': 1,\n",
       " 'implications': 1,\n",
       " 'web': 1,\n",
       " 'development': 1,\n",
       " 'incorporating': 1,\n",
       " 'capabilities': 1,\n",
       " 'surprising': 1,\n",
       " 'fact': 1,\n",
       " 'one': 1,\n",
       " 'elicit’s': 1,\n",
       " 'conditioned': 1,\n",
       " 'suggestions': 1,\n",
       " 'original': 1,\n",
       " 'itself': 1,\n",
       " 'wasn’t': 1,\n",
       " 'perfect': 1,\n",
       " 'reminded': 1,\n",
       " 'me': 1,\n",
       " 'revised': 1,\n",
       " 'accordingly': 1,\n",
       " 'scenario-planning': 1,\n",
       " 'exercises': 1,\n",
       " 'although': 1,\n",
       " 'relatively': 1,\n",
       " 'crude': 1,\n",
       " 'state': 1,\n",
       " 'bottom': 1,\n",
       " 'line': 1,\n",
       " 'encourage': 1,\n",
       " 'broad': 1,\n",
       " 'anticipate': 1,\n",
       " 'levels': 1,\n",
       " 'get': 1,\n",
       " 'yourselves': 1,\n",
       " 'bet': 1,\n",
       " 'boat': 1,\n",
       " 'out': 1,\n",
       " 'team': 1,\n",
       " 'gains': 1,\n",
       " 'then': 1,\n",
       " 'ahead': 1,\n",
       " 'competition': 1,\n",
       " 'remember': 1,\n",
       " 'underestimate': 1,\n",
       " 'ai.large': 1,\n",
       " 'exhibit': 1,\n",
       " 'abilities': 1,\n",
       " 'generalize': 1,\n",
       " 'without': 1,\n",
       " 'task-specific': 1,\n",
       " 'recent': 1,\n",
       " 'toward': 1,\n",
       " 'human-level': 1,\n",
       " 'generalization': 1,\n",
       " 'ultimate': 1,\n",
       " 'goals': 1,\n",
       " 'including': 1,\n",
       " 'those': 1,\n",
       " 'systems': 1,\n",
       " 'ai-driven': 1,\n",
       " 'explosive': 1,\n",
       " 'would': 1,\n",
       " 'skeptical': 1,\n",
       " 'prudent': 1,\n",
       " 'cognizant': 1,\n",
       " 'early': 1,\n",
       " 'signs': 1,\n",
       " 'consider': 1,\n",
       " 'former': 1,\n",
       " 'chief': 1,\n",
       " 'eric': 1,\n",
       " 'schmidt': 1,\n",
       " 'expects': 1,\n",
       " '10–20': 1,\n",
       " 'uk': 1,\n",
       " 'took': 1,\n",
       " 'official': 1,\n",
       " 'position': 1,\n",
       " 'paid': 1,\n",
       " 'anthony': 1,\n",
       " 'fauci’s': 1,\n",
       " '2017': 1,\n",
       " 'warning': 1,\n",
       " 'importance': 1,\n",
       " 'preparedness': 1,\n",
       " 'severe': 1,\n",
       " 'ensuing': 1,\n",
       " 'avoided': 1,\n",
       " 'ignoring': 1,\n",
       " 'carries': 1,\n",
       " 'inaction': 1,\n",
       " 'irresponsible': 1,\n",
       " 'widespread': 1,\n",
       " 'damaging': 1,\n",
       " 'e.g': 1,\n",
       " 'inequality': 1,\n",
       " 'domain-specific': 1,\n",
       " 'automation': 1,\n",
       " 'however': 1,\n",
       " 'unlike': 1,\n",
       " 'societal': 1,\n",
       " 'changes': 1,\n",
       " 'irreversible': 1,\n",
       " 'accelerate': 1,\n",
       " 'should': 1,\n",
       " 'capitalize': 1,\n",
       " 'avoid': 1,\n",
       " 'undesirable': 1,\n",
       " 'futures': 1,\n",
       " 'ensure': 1,\n",
       " 'equitably': 1,\n",
       " 'benefit': 1,\n",
       " 'stay': 1,\n",
       " 'powerful': 1,\n",
       " 'generalizable': 1,\n",
       " 'tip': 1,\n",
       " 'iceberg': 1,\n",
       " 'multimodal': 1,\n",
       " 'model-based': 1,\n",
       " 'involved': 1,\n",
       " 'aggressively': 1,\n",
       " 'quicker': 1,\n",
       " 'adjust': 1,\n",
       " 'move': 1,\n",
       " 'forget': 1,\n",
       " 'yourself': 1}"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_vocab(sents.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. Generate a document term matrix (DTM) as a numpy array\n",
    "\n",
    "\n",
    "Define a function `get_dtm(sents)` as follows:\n",
    "- accepts a list of sentences, i.e., `sents`, as an input\n",
    "- call `tokenize` function you defined in Q1 to get the count dictionary for each sentence, and combine them into a list\n",
    "- call `generate_vocab` function in Q2 to generate the large vocabulary for all sentences, and get all the words, i.e., keys\n",
    "- creates a numpy array, say `dtm` with a shape (# of docs x # of unique words), and set the initial values to 0.\n",
    "- fills cell `dtm[i,j]` with the count of the `j`th word in the `i`th sentence. HINT: you can loop through the list of vocabulary from step 2, and check each word's index in the large vocabulary from step 3, so that you can put the corresponding value into the correct cell. \n",
    "- returns `dtm` and `unique_words`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dtm(sents):\n",
    "    \n",
    "    all_docs = [tokenize(s) for s in sents]\n",
    "\n",
    "    all_words = list(generate_vocab(sents).keys())\n",
    "\n",
    "    m,n = len(all_docs), len(all_words)\n",
    "    dtm = np.zeros((m,n))\n",
    "    \n",
    "    for doc in range(m):\n",
    "        for i,word in enumerate(all_words):\n",
    "            if word in all_docs[doc]:\n",
    "                dtm[doc,i] = all_docs[doc][word]\n",
    "    \n",
    "    return dtm, all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ago', 1), ('ai-authored', 1), ('an', 1), ('and', 1), ('article', 1), ('been', 1), ('blog', 1), ('feats', 1), ('few', 1), ('for', 1), ('gone', 1), ('guardian', 1), ('has', 1), ('have', 1), ('it', 1), ('possible', 1), ('posts', 1), ('that', 1), ('the', 1), ('to', 1), ('used', 1), ('viral', 1), ('weren’t', 1), ('write', 1), ('years', 1)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "text    It has been used to write an article for The G...\n",
       "Name: 3, dtype: object"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ago', 1.0), ('ai-authored', 1.0), ('an', 1.0), ('and', 1.0), ('article', 1.0), ('been', 1.0), ('blog', 1.0), ('feats', 1.0), ('few', 1.0), ('for', 1.0), ('gone', 1.0), ('guardian', 1.0), ('has', 1.0), ('have', 1.0), ('it', 1.0), ('possible', 1.0), ('posts', 1.0), ('that', 1.0), ('the', 1.0), ('to', 1.0), ('used', 1.0), ('viral', 1.0), ('weren’t', 1.0), ('write', 1.0), ('years', 1.0)]\n"
     ]
    }
   ],
   "source": [
    "dtm, all_words = get_dtm(sents.text)\n",
    "\n",
    "# Check if the array is correct\n",
    "\n",
    "# randomly check one sentence\n",
    "idx = 3\n",
    "\n",
    "# get the dictionary using the function in Q1\n",
    "vocab = tokenize(sents[\"text\"].loc[idx])\n",
    "print(sorted(vocab.items(), key = lambda item: item[0]))\n",
    "\n",
    "# get all non-zero entries in dtm[idx] and create a dictionary\n",
    "# these two dictionaries should be the same\n",
    "sents.loc[idx]\n",
    "vocab1 ={all_words[j]: dtm[idx][j] for j in np.where(dtm[idx]>0)[0]}\n",
    "print(sorted(vocab1.items(), key = lambda item: item[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sorted(vocab.items(), key = lambda item: item[0])\n",
    "b = sorted(vocab1.items(), key = lambda item: item[0])\n",
    "\n",
    "assert a == b, f\"Dicts dont match\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4 Analyze DTM Array \n",
    "\n",
    "\n",
    "**Don't use any loop in this task**. You should use array operations to take the advantage of high performance computing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function named `analyze_dtm(dtm, words)` which:\n",
    "* takes an array $dtm$ and $words$ as an input, where $dtm$ is the array you get in Q3 with a shape $(m \\times n)$, and $words$ contains an array of words corresponding to the columns of $dtm$.\n",
    "* calculates the sentence frequency for each word, say $j$, e.g. how many sentences contain word $j$. Save the result to array $df$ ($df$ has shape of $(n,)$ or $(1, n)$).\n",
    "* normalizes the word count per sentence: divides word count, i.e., $dtm_{i,j}$, by the total number of words in sentence $i$. Save the result as an array named $tf$ ($tf$ has shape of $(m,n)$).\n",
    "* for each $dtm_{i,j}$, calculates $tf\\_idf_{i,j} = \\frac{tf_{i, j}}{df_j}$, i.e., divide each normalized word count by the sentence frequency of the word. The reason is, if a word appears in most sentences, it does not have the discriminative power and often is called a `stop` word. The inverse of $df$ can downgrade the weight of such words. $tf\\_idf$ has shape of $(m,n)$\n",
    "* prints out the following:\n",
    "    \n",
    "    - the total number of words in the document represented by $dtm$\n",
    "    - the most frequent top 10 words in this document, compare with the results from Q2, and briefly explain the difference \n",
    "    - words with the top 10 largest $df$ values (show words and their $df$ values)\n",
    "    - the longest sentence (i.e., the one with the most words)\n",
    "    - top-10 words with the largest $tf\\_idf$ values in the longest sentence (show words and values) \n",
    "* returns the $tf\\_idf$ array.\n",
    "\n",
    "\n",
    "\n",
    "Note, for all the steps, **do not use any loop**. Just use array functions and broadcasting for high performance computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dtm(dtm, words, sents):\n",
    "    \n",
    "    df = np.count_nonzero(dtm, axis=0)\n",
    "    \n",
    "    total = dtm.sum(axis=1)[:, np.newaxis]\n",
    "    tf = dtm/total\n",
    "    \n",
    "    tfidf = tf/df[np.newaxis,:]\n",
    "\n",
    "    n_top = 10\n",
    "    \n",
    "    words_freq = dtm.sum(axis=0)\n",
    "    words_most = words_freq.argsort()[::-1][:n_top]\n",
    "    top_words = list(zip(words[words_most], words_freq[words_most]))\n",
    "    \n",
    "    hi_df = df.argsort()[::-1][:n_top]\n",
    "    top_df = list(zip(words[hi_df], df[hi_df]))\n",
    "    \n",
    "    longest = dtm.sum(axis=1).argmax()\n",
    "    \n",
    "    longest_tfidf = tfidf[longest].argsort()[::-1][:n_top]\n",
    "    top_tfidf = list(zip(words[longest_tfidf], tfidf[longest][longest_tfidf]))\n",
    "    \n",
    "    print(f'The total number of words:\\n{dtm.sum()}\\n')\n",
    "    print(f'The top 10 frequent words:\\n{top_words}\\n')\n",
    "    print(f'The top 10 words with highest df values:\\n{top_df}\\n')\n",
    "    print(f'The longest sentence:\\n{sents[longest]}\\n')\n",
    "    print(f'The top 10 words with highest tf-idf values in the longest sentence:\\n{top_tfidf}')\n",
    "    \n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of words:\n",
      "1853.0\n",
      "\n",
      "The top 10 frequent words:\n",
      "[('the', 68.0), ('to', 65.0), ('and', 52.0), ('of', 50.0), ('for', 37.0), ('ai', 25.0), ('in', 24.0), ('is', 23.0), ('are', 22.0), ('be', 20.0)]\n",
      "\n",
      "The top 10 words with highest df values:\n",
      "[('the', 46), ('to', 42), ('and', 41), ('of', 36), ('for', 32), ('ai', 21), ('in', 21), ('like', 20), ('is', 20), ('tasks', 19)]\n",
      "\n",
      "The longest sentence:\n",
      "Language models are already reshaping traditional text analytics, but GPT-3 was an especially pivotal language model because, at 10x larger than any previous model upon release, it was the first large language model, which enabled it to perform even more advanced tasks like programming and solving high school–level math problems.\n",
      "\n",
      "The top 10 words with highest tf-idf values in the longest sentence:\n",
      "[('high', 0.02), ('math', 0.02), ('reshaping', 0.02), ('10x', 0.02), ('larger', 0.02), ('upon', 0.02), ('release', 0.02), ('enabled', 0.02), ('perform', 0.02), ('solving', 0.02)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.00362319, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.00074963, 0.00082102, 0.00084104, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.00088731, 0.        , 0.00049776, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.00094518, 0.0010352 , 0.00106045, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.00074963, 0.00164204, 0.00084104, ..., 0.03448276, 0.        ,\n",
       "        0.        ],\n",
       "       [0.00086957, 0.00285714, 0.00097561, ..., 0.        , 0.04      ,\n",
       "        0.04      ]])"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = np.array(all_words)\n",
    "\n",
    "analyze_dtm(dtm, words, sents.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus question (1 point)\n",
    "1. Suppose your machine learning model returns a list of probabilities as the output. Write a function to do the following:\n",
    "    - Given a threshold, say $th$, if a probability > $th$, the prediction is positive; otherwise, negative\n",
    "    - Compare the prediction with the ground truth labels to calculate the confusion matrix as [[TN, FN],[FP,TP]], where:\n",
    "        * True Positives (TP): the number of correct positive predictions\n",
    "        * False Positives (FP): the number of postive predictives which actually are negatives\n",
    "        * True Negatives (TN): the number of correct negative predictions\n",
    "        * False Negatives (FN): the number of negative predictives which actually are positives\n",
    "    - Calculate **precision** as $TP/(TP+FP)$ and **recall** as $TP/(TP+FN)$\n",
    "    - return precision and recall. \n",
    "2. Call this function with $th$ varying from 0.05 to 0.95 with an increase of 0.05. Plot a line chart to see how precision and recall change by $th$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob =np.array([0.28997326, 0.10166073, 0.10759583, 0.0694934 , 0.6767239 ,\n",
    "       0.01446897, 0.15268748, 0.15570522, 0.12159665, 0.22593857,\n",
    "       0.98162019, 0.47418329, 0.09376987, 0.80440782, 0.88361167,\n",
    "       0.21579844, 0.72343069, 0.06605903, 0.15447797, 0.10967575,\n",
    "       0.93020135, 0.06570391, 0.05283854, 0.09668829, 0.05974545,\n",
    "       0.04874688, 0.07562255, 0.11103822, 0.71674525, 0.08507381,\n",
    "       0.630128  , 0.16447478, 0.16914903, 0.1715767 , 0.08040751,\n",
    "       0.7001173 , 0.04428363, 0.19469664, 0.12247959, 0.14000294,\n",
    "       0.02411263, 0.26276603, 0.11377073, 0.07055441, 0.2021157 ,\n",
    "       0.11636899, 0.90348488, 0.10191679, 0.88744523, 0.18938904])\n",
    "\n",
    "truth = np.array([1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,\n",
    "       0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "       0, 0, 1, 0, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_performance(prob, truth, th):\n",
    "    \n",
    "    conf = [[0, 0], [0, 0]]\n",
    "    \n",
    "    classifiers = list(zip(prob, truth))\n",
    "    \n",
    "    for p,t in classifiers:\n",
    "        \n",
    "        guess = 0\n",
    "        \n",
    "        if p > th:\n",
    "            guess = 1\n",
    "        \n",
    "        if guess == t:\n",
    "                if guess == 0:\n",
    "                    conf[0][0] += 1 \n",
    "                else:\n",
    "                    conf[1][1] += 1\n",
    "                    \n",
    "        else:\n",
    "            if guess == 0:\n",
    "                conf[0][1] += 1\n",
    "            else:\n",
    "                conf[1][0] += 1\n",
    "    \n",
    "    [[TN,FN],[FP,TP]] = conf\n",
    "    prec = TP/(TP+FP)\n",
    "    rec = TP/(TP+FN)\n",
    "    \n",
    "    return {\"R0\":prec, \"R1\":rec}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'R0': 0.2608695652173913, 'R1': 1.0}"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test with one value\n",
    "evaluate_performance(prob, truth, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.05</th>\n",
       "      <td>0.260870</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.10</th>\n",
       "      <td>0.342857</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.15</th>\n",
       "      <td>0.480000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.20</th>\n",
       "      <td>0.705882</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>0.857143</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.30</th>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.35</th>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.40</th>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.45</th>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.55</th>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.60</th>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.65</th>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.70</th>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.80</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.85</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.95</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          prec       rec\n",
       "0.05  0.260870  1.000000\n",
       "0.10  0.342857  1.000000\n",
       "0.15  0.480000  1.000000\n",
       "0.20  0.705882  1.000000\n",
       "0.25  0.857143  1.000000\n",
       "0.30  0.916667  0.916667\n",
       "0.35  0.916667  0.916667\n",
       "0.40  0.916667  0.916667\n",
       "0.45  0.916667  0.916667\n",
       "0.50  0.909091  0.833333\n",
       "0.55  0.909091  0.833333\n",
       "0.60  0.909091  0.833333\n",
       "0.65  0.900000  0.750000\n",
       "0.70  0.888889  0.666667\n",
       "0.75  1.000000  0.500000\n",
       "0.80  1.000000  0.500000\n",
       "0.85  1.000000  0.416667\n",
       "0.90  1.000000  0.250000\n",
       "0.95  1.000000  0.083333"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test with threhold grid\n",
    "th_list=np.arange(0.05,1.00,0.05)\n",
    "\n",
    "vals = [tuple(evaluate_performance(prob, truth, th).values()) for th in th_list]\n",
    "prec, rec = zip(*vals)\n",
    "\n",
    "# can also make the plotting more concise with a dataframe\n",
    "vals_df = pd.DataFrame(vals, columns = ['prec','rec'], index = th_list)\n",
    "vals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x264adeb0af0>,\n",
       " <matplotlib.lines.Line2D at 0x264adeb0b20>]"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x264ade2bdf0>"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Precision and Recall vs. Threshold')"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# make a plot \n",
    "plt.plot(th_list, vals_df, label = ['prec','rec'])\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Precision and Recall vs. Threshold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
